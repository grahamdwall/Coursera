{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cognitiveclass.ai\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\"> </a>\n",
    "\n",
    "<h1 align=center><font size = 5>Pre-Trained Models</font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will learn how to leverage pre-trained models to build image classifiers instead of building a model from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 3> \n",
    "    \n",
    "1. <a href=\"#item31\">Import Libraries and Packages</a>\n",
    "2. <a href=\"#item32\">Download Data</a>  \n",
    "3. <a href=\"#item33\">Define Global Constants</a>  \n",
    "4. <a href=\"#item34\">Construct ImageDataGenerator Instances</a>  \n",
    "5. <a href=\"#item35\">Compile and Fit Model</a>\n",
    "\n",
    "</font>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item31'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start the lab by importing the libraries that we will be using in this lab. First we will need the library that helps us to import the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import skillsnetwork "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will import the ImageDataGenerator module since we will be leveraging it to train our model in batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will be using the Keras library to build an image classifier, so let's download the Keras library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will be leveraging the ResNet50 model to build our classifier, so let's download it as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item32'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you are going to download the data from IBM object storage using **skillsnetwork.prepare** command. skillsnetwork.prepare is a command that's used to download a zip file, unzip it and store it in a specified directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f40622ea36b4f798d8dd410853c2222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading concrete_data_week3.zip:   0%|          | 0/97863179 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "929aa22a383146ac8c95f7fa3aca2406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30036 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to '.'\n"
     ]
    }
   ],
   "source": [
    "## get the data\n",
    "await skillsnetwork.prepare(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/concrete_data_week3.zip\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should see the folder *concrete_data_week3* appear in the left pane. If you open this folder by double-clicking on it, you will find that it contains two folders: *train* and *valid*. And if you explore these folders, you will find that each contains two subfolders: *positive* and *negative*. These are the same folders that we saw in the labs in the previous modules of this course, where *negative* is the negative class and it represents the concrete images with no cracks and *positive* is the positive class and it represents the concrete images with cracks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note**: There are thousands and thousands of images in each folder, so please don't attempt to double click on the *negative* and *positive* folders. This may consume all of your memory and you may end up with a **50** error. So please **DO NOT DO IT**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item33'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Global Constants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will define constants that we will be using throughout the rest of the lab. \n",
    "\n",
    "1. We are obviously dealing with two classes, so *num_classes* is 2. \n",
    "2. The ResNet50 model was built and trained using images of size (224 x 224). Therefore, we will have to resize our images from (227 x 227) to (224 x 224).\n",
    "3. We will training and validating the model using batches of 100 images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "image_resize = 224\n",
    "\n",
    "batch_size_training = 100\n",
    "batch_size_validation = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item34'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct ImageDataGenerator Instances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to instantiate an ImageDataGenerator instance, we will set the **preprocessing_function** argument to *preprocess_input* which we imported from **keras.applications.resnet50** in order to preprocess our images the same way the images used to train ResNet50 model were processed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_generator = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the *flow_from_directory* method to get the training images as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/train',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_training,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: in this lab, we will be using the full data-set of 30,000 images for training and validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Turn**: Use the *flow_from_directory* method to get the validation images and assign the result to **validation_generator**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "## Type your answer here\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_training,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click __here__ for the solution.\n",
    "<!-- The correct answer is:\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_validation,\n",
    "    class_mode='categorical')\n",
    "-->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item35'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build, Compile and Fit Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will start building our model. We will use the Sequential model class from Keras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will add the ResNet50 pre-trained model to out model. However, note that we don't want to include the top layer or the output layer of the pre-trained model. We actually want to define our own output layer and train it so that it is optimized for our image dataset. In order to leave out the output layer of the pre-trained model, we will use the argument *include_top* and set it to **False**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:508: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3837: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:168: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:175: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1801: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3661: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 02:13:43.698736: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "2025-03-05 02:13:43.736086: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2394320000 Hz\n",
      "2025-03-05 02:13:43.736786: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x560f0f0eb790 executing computations on platform Host. Devices:\n",
      "2025-03-05 02:13:43.736956: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2025-03-05 02:13:43.817410: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3665: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94658560/94653016 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model.add(ResNet50(\n",
    "    include_top=False,\n",
    "    pooling='avg',\n",
    "    weights='imagenet',\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will define our output layer as a **Dense** layer, that consists of two nodes and uses the **Softmax** function as the activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the model's layers using the *layers* attribute of our model object. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.training.Model at 0x7f1b3a5e5190>,\n",
       " <keras.layers.core.Dense at 0x7f1b3831c390>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that our model is composed of two sets of layers. The first set is the layers pertaining to ResNet50 and the second set is a single layer, which is our Dense layer that we defined above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the ResNet50 layers by running the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.topology.InputLayer at 0x7f1bc3a00dd0>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f1c0017b990>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1bb7c6a990>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1c00146d10>,\n",
       " <keras.layers.core.Activation at 0x7f1bbdd19750>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f1bbd35d710>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1bbd2b29d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1bbd257710>,\n",
       " <keras.layers.core.Activation at 0x7f1bbd257290>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1bbd1ebc50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1bbd1d8c10>,\n",
       " <keras.layers.core.Activation at 0x7f1bbd1d8e10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1bb4427ed0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1bb4324a10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1bb438edd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1bb42dc050>,\n",
       " <keras.layers.merge.Add at 0x7f1bb42dcfd0>,\n",
       " <keras.layers.core.Activation at 0x7f1bb422acd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1bb41db810>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1bb41a5cd0>,\n",
       " <keras.layers.core.Activation at 0x7f1bb41a5f50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1bb40e1dd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1bb40b4410>,\n",
       " <keras.layers.core.Activation at 0x7f1bb40b4fd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1b947916d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1b9470bf50>,\n",
       " <keras.layers.merge.Add at 0x7f1b94727250>,\n",
       " <keras.layers.core.Activation at 0x7f1b946aae10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1b9465c9d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1b94625a90>,\n",
       " <keras.layers.core.Activation at 0x7f1b945ebf90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1b945a8f50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1b944c81d0>,\n",
       " <keras.layers.core.Activation at 0x7f1b94521410>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1b9443e250>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1b94436f10>,\n",
       " <keras.layers.merge.Add at 0x7f1b943d4210>,\n",
       " <keras.layers.core.Activation at 0x7f1b94359d90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1b942e1e90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1b942cfad0>,\n",
       " <keras.layers.core.Activation at 0x7f1b942ba210>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1b94250e50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1b941ca110>,\n",
       " <keras.layers.core.Activation at 0x7f1b941f2150>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1b94140810>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1b9405abd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1b94088e90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1b94077810>,\n",
       " <keras.layers.merge.Add at 0x7f1b84700510>,\n",
       " <keras.layers.core.Activation at 0x7f1b84715f10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1b846b8a50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1b84693f50>,\n",
       " <keras.layers.core.Activation at 0x7f1b84693d10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1b84633d90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1b84598710>,\n",
       " <keras.layers.core.Activation at 0x7f1b84598f10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1b84530bd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1b844a6150>,\n",
       " <keras.layers.merge.Add at 0x7f1b844a6210>,\n",
       " <keras.layers.core.Activation at 0x7f1b843c6790>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1b843e9a90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1b843ac5d0>,\n",
       " <keras.layers.core.Activation at 0x7f1b84340d50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1b842dfe90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1b8426c210>,\n",
       " <keras.layers.core.Activation at 0x7f1b84244650>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1b841dc950>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1b84155d50>,\n",
       " <keras.layers.merge.Add at 0x7f1b841550d0>,\n",
       " <keras.layers.core.Activation at 0x7f1b840f39d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1b687c5150>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1b8406bb10>,\n",
       " <keras.layers.core.Activation at 0x7f1b8406bf90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1b6874de50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1b686d8190>,\n",
       " <keras.layers.core.Activation at 0x7f1b6872f690>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1b686b0710>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1b6858ea90>,\n",
       " <keras.layers.merge.Add at 0x7f1b685c5190>,\n",
       " <keras.layers.core.Activation at 0x7f1b6855f110>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1b6855f910>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1b684c1b10>,\n",
       " <keras.layers.core.Activation at 0x7f1b684d5fd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1b683f9d50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1b68395410>,\n",
       " <keras.layers.core.Activation at 0x7f1b683d8110>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1b68371e50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1b6820c5d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1b682dfa90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1b6822db10>,\n",
       " <keras.layers.merge.Add at 0x7f1b681c0b90>,\n",
       " <keras.layers.core.Activation at 0x7f1b68112cd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1b680b35d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1b6808cdd0>,\n",
       " <keras.layers.core.Activation at 0x7f1b6808c590>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1b3bf833d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1b3bf66e90>,\n",
       " <keras.layers.core.Activation at 0x7f1b3bf66b50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1b3be85c50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1b3be7b3d0>,\n",
       " <keras.layers.merge.Add at 0x7f1b3be7b6d0>,\n",
       " <keras.layers.core.Activation at 0x7f1b3bd98990>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1b3bd98c90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1b3bd14210>,\n",
       " <keras.layers.core.Activation at 0x7f1b3bd14150>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1b3bcaf950>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1b3bc29d50>,\n",
       " <keras.layers.core.Activation at 0x7f1b3bc290d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1b3bb475d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1b3bb28c10>,\n",
       " <keras.layers.merge.Add at 0x7f1b3bb28550>,\n",
       " <keras.layers.core.Activation at 0x7f1b3ba60bd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1b3bcafdd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1b3b9c0f90>,\n",
       " <keras.layers.core.Activation at 0x7f1b3b9c00d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1b3b976d90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1b3b8da910>,\n",
       " <keras.layers.core.Activation at 0x7f1b3b8dae90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1b3b871cd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1b3b7ea110>,\n",
       " <keras.layers.merge.Add at 0x7f1b3b7ea1d0>,\n",
       " <keras.layers.core.Activation at 0x7f1b3b709750>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1b3b976610>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1b3b6efa90>,\n",
       " <keras.layers.core.Activation at 0x7f1b3b680950>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1b3b61cb10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1b3b587ed0>,\n",
       " <keras.layers.core.Activation at 0x7f1b3b587d50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1b3b538d90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1b3b49e990>,\n",
       " <keras.layers.merge.Add at 0x7f1b3b49efd0>,\n",
       " <keras.layers.core.Activation at 0x7f1b3b436d10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1b3b3d86d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1b3b3adc50>,\n",
       " <keras.layers.core.Activation at 0x7f1b3b3ad5d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1b3b2e5ad0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1b3b26ddd0>,\n",
       " <keras.layers.core.Activation at 0x7f1b3b245490>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1b3b1e1250>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1b3b14bf90>,\n",
       " <keras.layers.merge.Add at 0x7f1b3b14be10>,\n",
       " <keras.layers.core.Activation at 0x7f1b3b0fa690>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1b3b1e1a50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1b3b05bf10>,\n",
       " <keras.layers.core.Activation at 0x7f1b3b075d50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1b3af91750>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1b3af74f90>,\n",
       " <keras.layers.core.Activation at 0x7f1b3af74c50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1b3aea8b10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1b3ada6ad0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1b3ae09610>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1b3ad5c110>,\n",
       " <keras.layers.merge.Add at 0x7f1b3aca7590>,\n",
       " <keras.layers.core.Activation at 0x7f1b3acbb310>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1b3acbb410>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1b3ac38f90>,\n",
       " <keras.layers.core.Activation at 0x7f1b3ac38d90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1b3ab55050>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1b3ab3ea10>,\n",
       " <keras.layers.core.Activation at 0x7f1b3ab3ee10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1b3aa6c1d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1b3a9cfa90>,\n",
       " <keras.layers.merge.Add at 0x7f1b3a9cf3d0>,\n",
       " <keras.layers.core.Activation at 0x7f1b3a96dc50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1b3a96d610>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1b3a8e6350>,\n",
       " <keras.layers.core.Activation at 0x7f1b3a8e6450>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1b3a820fd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1b3a7b8390>,\n",
       " <keras.layers.core.Activation at 0x7f1b3a781f90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1b3a718810>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1b3a684a50>,\n",
       " <keras.layers.merge.Add at 0x7f1b3a684e50>,\n",
       " <keras.layers.core.Activation at 0x7f1b3a630110>,\n",
       " <keras.layers.pooling.AveragePooling2D at 0x7f1b3a96d8d0>,\n",
       " <keras.layers.pooling.GlobalAveragePooling2D at 0x7f1bbd2c5450>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the ResNet50 model has already been trained, then we want to tell our model not to bother with training the ResNet part, but to train only our dense output layer. To do that, we run the following.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now using the *summary* attribute of the model, we can see how many parameters we will need to optimize in order to train the output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 2048)              23587712  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 4098      \n",
      "=================================================================\n",
      "Total params: 23,591,810\n",
      "Trainable params: 4,098\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compile our model using the **adam** optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/optimizers.py:757: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we are able to start the training process, with an ImageDataGenerator, we will need to define how many steps compose an epoch. Typically, that is the number of images divided by the batch size. Therefore, we define our steps per epoch as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "steps_per_epoch_training = len(train_generator)\n",
    "steps_per_epoch_validation = len(validation_generator)\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to start training our model. Unlike a conventional deep learning training were data is not streamed from a directory, with an ImageDataGenerator where data is augmented in batches, we use the **fit_generator** method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/2\n",
      " 61/101 [=================>............] - ETA: 44:27 - loss: 0.1327 - acc: 0.9521"
     ]
    }
   ],
   "source": [
    "fit_history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch_training,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=steps_per_epoch_validation,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, you are ready to start using it to classify images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since training can take a long time when building deep learning models, it is always a good idea to save your model once the training is complete if you believe you will be using the model again later. You will be using this model in the next module, so go ahead and save your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save('classifier_resnet_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should see the model file *classifier_resnet_model.h5* apprear in the left directory pane.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank you for completing this lab!\n",
    "\n",
    "This notebook was created by Alex Aklson. I hope you found this lab interesting and educational.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of a course on **Coursera** called *AI Capstone Project with Deep Learning*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0321EN_Coursera_Week3_LAB1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2020-09-18  | 2.0  | Shubham  |  Migrated Lab to Markdown and added to course repo in GitLab |\n",
    "| 2023-01-03  | 3.0  | Artem |  Updated the file import section|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Copyright &copy; 2020 [IBM Developer Skills Network](https://cognitiveclass.ai/?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "prev_pub_hash": "cf2970a1d2c549fe86023eaa076d0ce4936c4275baf2cccfdad8fe6ce3a8a6c2"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
