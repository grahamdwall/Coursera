{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab: Custom Training Loops in Keras**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **30** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will learn to implement a basic custom training loop in Keras. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "By the end of this lab, you will: \n",
    "\n",
    "- Set up the environment \n",
    "\n",
    "- Define the neural network model \n",
    "\n",
    "- Define the Loss Function and Optimizer \n",
    "\n",
    "- Implement the custom training loop \n",
    "\n",
    "- Enhance the custom training loop by adding an accuracy metric to monitor model performance \n",
    "\n",
    "- Implement a custom callback to log additional metrics and information during training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-by-Step Instructions:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Basic custom training loop: \n",
    "\n",
    "#### 1. Set Up the Environment:\n",
    "\n",
    "- Import necessary libraries. \n",
    "\n",
    "- Load and preprocess the MNIST dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.18.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from tensorflow) (24.2)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from tensorflow) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (4.12.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.70.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow)\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Downloading keras-3.8.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Downloading h5py-3.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.4.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\n",
      "  Downloading optree-0.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.19,>=2.18->tensorflow)\n",
      "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.19,>=2.18->tensorflow)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow-2.18.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m615.5/615.5 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.70.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.8.0-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.4.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Downloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (89 kB)\n",
      "Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (403 kB)\n",
      "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorboard-data-server, protobuf, optree, opt-einsum, numpy, mdurl, markdown, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, ml-dtypes, markdown-it-py, h5py, rich, keras, tensorflow\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 grpcio-1.70.0 h5py-3.13.0 keras-3.8.0 libclang-18.1.1 markdown-3.7 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.4.1 namex-0.0.8 numpy-2.0.2 opt-einsum-3.4.0 optree-0.14.1 protobuf-5.29.3 rich-13.9.4 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 termcolor-2.5.0 werkzeug-3.1.3 wrapt-1.17.2\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import numpy as np\n",
    "\n",
    "# Suppress all Python warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set TensorFlow log level to suppress warnings and info messages\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Step 1: Set Up the Environment\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() \n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Define the model: \n",
    "\n",
    "Create a simple neural network model with a Flatten layer followed by two Dense layers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define the Model\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Define Loss Function and Optimizer: \n",
    "\n",
    "- Use Sparse Categorical Crossentropy for the loss function. \n",
    "- Use the Adam optimizer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define Loss Function and Optimizer\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) \n",
    "optimizer = tf.keras.optimizers.Adam()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Implement the Custom Training Loop: \n",
    "\n",
    "- Iterate over the dataset for a specified number of epochs. \n",
    "- Compute the loss and apply gradients to update the model's weights. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 1\n",
      "Epoch 1 Step 0: Loss = 2.428905963897705\n",
      "Epoch 1 Step 200: Loss = 0.39243635535240173\n",
      "Epoch 1 Step 400: Loss = 0.17183054983615875\n",
      "Epoch 1 Step 600: Loss = 0.18859410285949707\n",
      "Epoch 1 Step 800: Loss = 0.18477171659469604\n",
      "Epoch 1 Step 1000: Loss = 0.4252595007419586\n",
      "Epoch 1 Step 1200: Loss = 0.20113474130630493\n",
      "Epoch 1 Step 1400: Loss = 0.25010278820991516\n",
      "Epoch 1 Step 1600: Loss = 0.20249883830547333\n",
      "Epoch 1 Step 1800: Loss = 0.19015295803546906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 13:36:05.496687: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 2\n",
      "Epoch 2 Step 0: Loss = 0.08896055817604065\n",
      "Epoch 2 Step 200: Loss = 0.17373313009738922\n",
      "Epoch 2 Step 400: Loss = 0.09065383672714233\n",
      "Epoch 2 Step 600: Loss = 0.06709770858287811\n",
      "Epoch 2 Step 800: Loss = 0.11685469001531601\n",
      "Epoch 2 Step 1000: Loss = 0.22509993612766266\n",
      "Epoch 2 Step 1200: Loss = 0.08535180985927582\n",
      "Epoch 2 Step 1400: Loss = 0.13548152148723602\n",
      "Epoch 2 Step 1600: Loss = 0.14023704826831818\n",
      "Epoch 2 Step 1800: Loss = 0.09223531186580658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 13:37:00.963891: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Implement the Custom Training Loop\n",
    "\n",
    "epochs = 2\n",
    "# train_dataset = train_dataset.repeat(epochs)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "for epoch in range(epochs):\n",
    "    print(f'Start of epoch {epoch + 1}')\n",
    "\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train, training=True)  # Forward pass\n",
    "            loss_value = loss_fn(y_batch_train, logits)  # Compute loss\n",
    "\n",
    "        # Compute gradients and update weights\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # Logging the loss every 200 steps\n",
    "        if step % 200 == 0:\n",
    "            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Adding Accuracy Metric:\n",
    "\n",
    "Enhance the custom training loop by adding an accuracy metric to monitor model performance. \n",
    "\n",
    "#### 1. Set Up the Environment: \n",
    "\n",
    "Follow the setup from Exercise 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Flatten \n",
    "\n",
    "# Step 1: Set Up the Environment\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values to be between 0 and 1\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 \n",
    "\n",
    "# Create a batched dataset for training\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Define the Model: \n",
    "Use the same model as in Exercise 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define the Model\n",
    "\n",
    "model = Sequential([ \n",
    "    Flatten(input_shape=(28, 28)),  # Flatten the input to a 1D vector\n",
    "    Dense(128, activation='relu'),  # First hidden layer with 128 neurons and ReLU activation\n",
    "    Dense(10)  # Output layer with 10 neurons for the 10 classes (digits 0-9)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Define the loss function, optimizer, and metric: \n",
    "\n",
    "- Use Sparse Categorical Crossentropy for the loss function and Adam optimizer. \n",
    "\n",
    "- Add Sparse Categorical Accuracy as a metric. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define Loss Function, Optimizer, and Metric\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Loss function for multi-class classification\n",
    "optimizer = tf.keras.optimizers.Adam()  # Adam optimizer for efficient training\n",
    "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy()  # Metric to track accuracy during training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Implement the custom training loop with accuracy: \n",
    "\n",
    "Track the accuracy during training and print it at regular intervals. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 1\n",
      "Epoch 1 Step 0: Loss = 2.2686030864715576 Accuracy = 0.03125\n",
      "Epoch 1 Step 200: Loss = 0.3515393137931824 Accuracy = 0.8350435495376587\n",
      "Epoch 1 Step 400: Loss = 0.19776566326618195 Accuracy = 0.8679083585739136\n",
      "Epoch 1 Step 600: Loss = 0.17220601439476013 Accuracy = 0.8837354183197021\n",
      "Epoch 1 Step 800: Loss = 0.13083554804325104 Accuracy = 0.896691620349884\n",
      "Epoch 1 Step 1000: Loss = 0.4188894033432007 Accuracy = 0.9034090638160706\n",
      "Epoch 1 Step 1200: Loss = 0.14457967877388 Accuracy = 0.9096065759658813\n",
      "Epoch 1 Step 1400: Loss = 0.2598384618759155 Accuracy = 0.9145699739456177\n",
      "Epoch 1 Step 1600: Loss = 0.19589580595493317 Accuracy = 0.9178052544593811\n",
      "Epoch 1 Step 1800: Loss = 0.13362674415111542 Accuracy = 0.9218142628669739\n",
      "Start of epoch 2\n",
      "Epoch 2 Step 0: Loss = 0.08986447006464005 Accuracy = 1.0\n",
      "Epoch 2 Step 200: Loss = 0.1578063815832138 Accuracy = 0.9603545069694519\n",
      "Epoch 2 Step 400: Loss = 0.08748961240053177 Accuracy = 0.9569825530052185\n",
      "Epoch 2 Step 600: Loss = 0.050890401005744934 Accuracy = 0.9590786099433899\n",
      "Epoch 2 Step 800: Loss = 0.06955215334892273 Accuracy = 0.9611813426017761\n",
      "Epoch 2 Step 1000: Loss = 0.21133315563201904 Accuracy = 0.9618818759918213\n",
      "Epoch 2 Step 1200: Loss = 0.09919170290231705 Accuracy = 0.9628174304962158\n",
      "Epoch 2 Step 1400: Loss = 0.16233284771442413 Accuracy = 0.9637535810470581\n",
      "Epoch 2 Step 1600: Loss = 0.16551755368709564 Accuracy = 0.9637726545333862\n",
      "Epoch 2 Step 1800: Loss = 0.07049968838691711 Accuracy = 0.9647418260574341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 13:40:04.531998: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 3\n",
      "Epoch 3 Step 0: Loss = 0.041741110384464264 Accuracy = 1.0\n",
      "Epoch 3 Step 200: Loss = 0.13975007832050323 Accuracy = 0.9743469953536987\n",
      "Epoch 3 Step 400: Loss = 0.08783796429634094 Accuracy = 0.9725685715675354\n",
      "Epoch 3 Step 600: Loss = 0.042844757437705994 Accuracy = 0.9737936854362488\n",
      "Epoch 3 Step 800: Loss = 0.056785207241773605 Accuracy = 0.9745240211486816\n",
      "Epoch 3 Step 1000: Loss = 0.1355559229850769 Accuracy = 0.9755244851112366\n",
      "Epoch 3 Step 1200: Loss = 0.05670663341879845 Accuracy = 0.9759575128555298\n",
      "Epoch 3 Step 1400: Loss = 0.08712360262870789 Accuracy = 0.9761108160018921\n",
      "Epoch 3 Step 1600: Loss = 0.10618353635072708 Accuracy = 0.9759134650230408\n",
      "Epoch 3 Step 1800: Loss = 0.040258489549160004 Accuracy = 0.976263165473938\n",
      "Start of epoch 4\n",
      "Epoch 4 Step 0: Loss = 0.027191421017050743 Accuracy = 1.0\n",
      "Epoch 4 Step 200: Loss = 0.09671405702829361 Accuracy = 0.9810323119163513\n",
      "Epoch 4 Step 400: Loss = 0.08293671160936356 Accuracy = 0.9805174469947815\n",
      "Epoch 4 Step 600: Loss = 0.04821016639471054 Accuracy = 0.9815411567687988\n",
      "Epoch 4 Step 800: Loss = 0.052919935435056686 Accuracy = 0.9818196296691895\n",
      "Epoch 4 Step 1000: Loss = 0.11859100311994553 Accuracy = 0.982361376285553\n",
      "Epoch 4 Step 1200: Loss = 0.029398636892437935 Accuracy = 0.9825926423072815\n",
      "Epoch 4 Step 1400: Loss = 0.04285192862153053 Accuracy = 0.9827355742454529\n",
      "Epoch 4 Step 1600: Loss = 0.054951261729002 Accuracy = 0.9826865792274475\n",
      "Epoch 4 Step 1800: Loss = 0.02285093069076538 Accuracy = 0.9829608798027039\n",
      "Start of epoch 5\n",
      "Epoch 5 Step 0: Loss = 0.023140888661146164 Accuracy = 1.0\n",
      "Epoch 5 Step 200: Loss = 0.05846614018082619 Accuracy = 0.9872512221336365\n",
      "Epoch 5 Step 400: Loss = 0.07055129110813141 Accuracy = 0.9857387542724609\n",
      "Epoch 5 Step 600: Loss = 0.034663453698158264 Accuracy = 0.9869488477706909\n",
      "Epoch 5 Step 800: Loss = 0.031924329698085785 Accuracy = 0.9871254563331604\n",
      "Epoch 5 Step 1000: Loss = 0.10379665344953537 Accuracy = 0.9876061677932739\n",
      "Epoch 5 Step 1200: Loss = 0.020940175279974937 Accuracy = 0.9875103831291199\n",
      "Epoch 5 Step 1400: Loss = 0.019382420927286148 Accuracy = 0.9877097010612488\n",
      "Epoch 5 Step 1600: Loss = 0.027404339984059334 Accuracy = 0.9878005981445312\n",
      "Epoch 5 Step 1800: Loss = 0.024041369557380676 Accuracy = 0.9879927635192871\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Implement the Custom Training Loop with Accuracy\n",
    "\n",
    "epochs = 5  # Number of epochs for training\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Start of epoch {epoch + 1}')\n",
    "    \n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass: Compute predictions\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            # Compute loss\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "        \n",
    "        # Compute gradients\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        # Apply gradients to update model weights\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        # Update the accuracy metric\n",
    "        accuracy_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "        # Log the loss and accuracy every 200 steps\n",
    "        if step % 200 == 0:\n",
    "            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy()} Accuracy = {accuracy_metric.result().numpy()}')\n",
    "    \n",
    "    # Reset the metric at the end of each epoch\n",
    "    accuracy_metric.reset_state()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Custom Callback for Advanced Logging: \n",
    "\n",
    "Implement a custom callback to log additional metrics and information during training. \n",
    "\n",
    "#### 1. Set Up the Environment: \n",
    "\n",
    "Follow the setup from Exercise 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Flatten \n",
    "\n",
    "# Step 1: Set Up the Environment\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values to be between 0 and 1\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 \n",
    "\n",
    "# Create a batched dataset for training\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Define the Model: \n",
    "\n",
    "Use the same model as in Exercise 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define the Model\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),  # Flatten the input to a 1D vector\n",
    "    Dense(128, activation='relu'),  # First hidden layer with 128 neurons and ReLU activation\n",
    "    Dense(10)  # Output layer with 10 neurons for the 10 classes (digits 0-9)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Define Loss Function, Optimizer, and Metric: \n",
    "\n",
    "- Use Sparse Categorical Crossentropy for the loss function and Adam optimizer. \n",
    "\n",
    "- Add Sparse Categorical Accuracy as a metric. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define Loss Function, Optimizer, and Metric\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Loss function for multi-class classification\n",
    "optimizer = tf.keras.optimizers.Adam()  # Adam optimizer for efficient training\n",
    "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy()  # Metric to track accuracy during training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Implement the custom training loop with custom callback: \n",
    "\n",
    "Create a custom callback to log additional metrics at the end of each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "# Step 4: Implement the Custom Callback \n",
    "class CustomCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        print(f'End of epoch {epoch + 1}, loss: {logs.get(\"loss\")}, accuracy: {logs.get(\"accuracy\")}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 1\n",
      "Epoch 1 Step 0: Loss = 2.289888620376587 Accuracy = 0.09375\n",
      "Epoch 1 Step 200: Loss = 0.3680278956890106 Accuracy = 0.8333333134651184\n",
      "Epoch 1 Step 400: Loss = 0.18024899065494537 Accuracy = 0.866583526134491\n",
      "Epoch 1 Step 600: Loss = 0.25739917159080505 Accuracy = 0.8832154870033264\n",
      "Epoch 1 Step 800: Loss = 0.15282343327999115 Accuracy = 0.8957163095474243\n",
      "Epoch 1 Step 1000: Loss = 0.42680424451828003 Accuracy = 0.9025974273681641\n",
      "Epoch 1 Step 1200: Loss = 0.16171634197235107 Accuracy = 0.9090861678123474\n",
      "Epoch 1 Step 1400: Loss = 0.2632545828819275 Accuracy = 0.9142130613327026\n",
      "Epoch 1 Step 1600: Loss = 0.2139626145362854 Accuracy = 0.9173953533172607\n",
      "Epoch 1 Step 1800: Loss = 0.17573893070220947 Accuracy = 0.9211202263832092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 13:50:11.031141: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 1, loss: 0.04563339799642563, accuracy: 0.9231166839599609\n",
      "Start of epoch 2\n",
      "Epoch 2 Step 0: Loss = 0.07137185335159302 Accuracy = 1.0\n",
      "Epoch 2 Step 200: Loss = 0.17205692827701569 Accuracy = 0.959266185760498\n",
      "Epoch 2 Step 400: Loss = 0.10917454957962036 Accuracy = 0.9563590884208679\n",
      "Epoch 2 Step 600: Loss = 0.049260079860687256 Accuracy = 0.9589226245880127\n",
      "Epoch 2 Step 800: Loss = 0.06803486496210098 Accuracy = 0.9601669907569885\n",
      "Epoch 2 Step 1000: Loss = 0.3081034719944 Accuracy = 0.9605082273483276\n",
      "Epoch 2 Step 1200: Loss = 0.07969001680612564 Accuracy = 0.9615164399147034\n",
      "Epoch 2 Step 1400: Loss = 0.18291153013706207 Accuracy = 0.9624598622322083\n",
      "Epoch 2 Step 1600: Loss = 0.14809858798980713 Accuracy = 0.9625429511070251\n",
      "Epoch 2 Step 1800: Loss = 0.10560812056064606 Accuracy = 0.9634925127029419\n",
      "End of epoch 2, loss: 0.025905556976795197, accuracy: 0.9641666412353516\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Implement the Custom Training Loop with Custom Callback\n",
    "\n",
    "epochs = 2\n",
    "custom_callback = CustomCallback()  # Initialize the custom callback\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Start of epoch {epoch + 1}')\n",
    "    \n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass: Compute predictions\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            # Compute loss\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "        \n",
    "        # Compute gradients\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        # Apply gradients to update model weights\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        # Update the accuracy metric\n",
    "        accuracy_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "        # Log the loss and accuracy every 200 steps\n",
    "        if step % 200 == 0:\n",
    "            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy()} Accuracy = {accuracy_metric.result().numpy()}')\n",
    "    \n",
    "    # Call the custom callback at the end of each epoch\n",
    "    custom_callback.on_epoch_end(epoch, logs={'loss': loss_value.numpy(), 'accuracy': accuracy_metric.result().numpy()})\n",
    "    \n",
    "    # Reset the metric at the end of each epoch\n",
    "    accuracy_metric.reset_state()  # Use reset_state() instead of reset_states()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Add Hidden Layers \n",
    "\n",
    "Next, you will add a couple of hidden layers to your model. Hidden layers help the model learn complex patterns in the data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "# Define the input layer\n",
    "input_layer = Input(shape=(28, 28))  # Input layer with shape (28, 28)\n",
    "\n",
    "# Define hidden layers\n",
    "hidden_layer1 = Dense(64, activation='relu')(input_layer)  # First hidden layer with 64 neurons and ReLU activation\n",
    "hidden_layer2 = Dense(64, activation='relu')(hidden_layer1)  # Second hidden layer with 64 neurons and ReLU activation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "`Dense(64, activation='relu')` creates a dense (fully connected) layer with 64 units and ReLU activation function. \n",
    "\n",
    "Each hidden layer takes the output of the previous layer as its input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Define the output layer \n",
    "\n",
    "Finally, you will define the output layer. Suppose you are working on a binary classification problem, so the output layer will have one unit with a sigmoid activation function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer = Dense(1, activation='sigmoid')(hidden_layer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "`Dense(1, activation='sigmoid')` creates a dense layer with 1 unit and a sigmoid activation function, suitable for binary classification. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Create the Model \n",
    "\n",
    "Now, you will create the model by specifying the input and output layers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "`Model(inputs=input_layer, outputs=output_layer)` creates a Keras model that connects the input layer to the output layer through the hidden layers. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: Compile the Model \n",
    "\n",
    "Before training the model, you need to compile it. You will specify the loss function, optimizer, and evaluation metrics. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "`optimizer='adam'` specifies the Adam optimizer, a popular choice for training neural networks. \n",
    "\n",
    "`loss='binary_crossentropy'` specifies the loss function for binary classification problems. \n",
    "\n",
    "`metrics=['accuracy']` tells Keras to evaluate the model using accuracy during training. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8: Train the Model \n",
    "\n",
    "You can now train the model on some training data. For this example, let's assume `X_train` is our training input data and `y_train` is the corresponding labels. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.4776 - loss: 0.7052  \n",
      "Epoch 2/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5300 - loss: 0.6908 \n",
      "Epoch 3/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5407 - loss: 0.6865 \n",
      "Epoch 4/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5436 - loss: 0.6869 \n",
      "Epoch 5/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5663 - loss: 0.6826 \n",
      "Epoch 6/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5464 - loss: 0.6871 \n",
      "Epoch 7/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5624 - loss: 0.6804 \n",
      "Epoch 8/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5493 - loss: 0.6824 \n",
      "Epoch 9/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5523 - loss: 0.6838 \n",
      "Epoch 10/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5383 - loss: 0.6844 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f26f0531670>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Redefine the Model for 20 features\n",
    "model = Sequential([\n",
    "    Input(shape=(20,)),  # Adjust input shape to (20,)\n",
    "    Dense(128, activation='relu'),  # Hidden layer with 128 neurons and ReLU activation\n",
    "    Dense(1, activation='sigmoid')  # Output layer for binary classification with sigmoid activation\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 2: Generate Example Data\n",
    "X_train = np.random.rand(1000, 20)  # 1000 samples, 20 features each\n",
    "y_train = np.random.randint(2, size=(1000, 1))  # 1000 binary labels (0 or 1)\n",
    "\n",
    "# Step 3: Train the Model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "`X_train` and `y_train` are placeholders for your actual training data. \n",
    "\n",
    "`model.fit` trains the model for a specified number of epochs and batch size. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9: Evaluate the Model \n",
    "\n",
    "After training, you can evaluate the model on test data to see how well it performs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4714 - loss: 0.7196  \n",
      "Test loss: 0.7147642374038696\n",
      "Test accuracy: 0.4749999940395355\n"
     ]
    }
   ],
   "source": [
    "# Example test data (in practice, use real dataset)\n",
    "X_test = np.random.rand(200, 20)  # 200 samples, 20 features each\n",
    "y_test = np.random.randint(2, size=(200, 1))  # 200 binary labels (0 or 1)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Print test loss and accuracy\n",
    "print(f'Test loss: {loss}')\n",
    "print(f'Test accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "`model.evaluate` computes the loss and accuracy of the model on test data. \n",
    "\n",
    "`X_test` and `y_test` are placeholders for your actual test data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises \n",
    "\n",
    "### Exercise 1: Basic Custom Training Loop \n",
    "\n",
    "#### Objective: Implement a basic custom training loop to train a simple neural network on the MNIST dataset. \n",
    "\n",
    "#### Instructions: \n",
    "\n",
    "- Set up the environment and load the dataset. \n",
    "\n",
    "- Define the model with a Flatten layer and two Dense layers. \n",
    "\n",
    "- Define the loss function and optimizer. \n",
    "\n",
    "- Implement a custom training loop to iterate over the dataset, compute the loss, and update the model's weights. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¹ Start of epoch 1\n",
      "Epoch 1 Step 0: Loss = 2.4115, Accuracy = 0.0625\n",
      "Epoch 1 Step 200: Loss = 0.3799, Accuracy = 0.8385\n",
      "Epoch 1 Step 400: Loss = 0.1774, Accuracy = 0.8696\n",
      "Epoch 1 Step 600: Loss = 0.1765, Accuracy = 0.8837\n",
      "Epoch 1 Step 800: Loss = 0.1575, Accuracy = 0.8965\n",
      "Epoch 1 Step 1000: Loss = 0.4422, Accuracy = 0.9035\n",
      "Epoch 1 Step 1200: Loss = 0.1968, Accuracy = 0.9099\n",
      "Epoch 1 Step 1400: Loss = 0.2247, Accuracy = 0.9150\n",
      "Epoch 1 Step 1600: Loss = 0.2241, Accuracy = 0.9179\n",
      "Epoch 1 Step 1800: Loss = 0.2319, Accuracy = 0.9221\n",
      "End of epoch 1, loss: 0.033201638609170914, accuracy: 0.924049973487854\n",
      "\n",
      "ğŸ”¹ Start of epoch 2\n",
      "Epoch 2 Step 0: Loss = 0.0985, Accuracy = 0.9688\n",
      "Epoch 2 Step 200: Loss = 0.1842, Accuracy = 0.9607\n",
      "Epoch 2 Step 400: Loss = 0.1429, Accuracy = 0.9568\n",
      "Epoch 2 Step 600: Loss = 0.0545, Accuracy = 0.9593\n",
      "Epoch 2 Step 800: Loss = 0.0994, Accuracy = 0.9608\n",
      "Epoch 2 Step 1000: Loss = 0.2370, Accuracy = 0.9612\n",
      "Epoch 2 Step 1200: Loss = 0.1399, Accuracy = 0.9620\n",
      "Epoch 2 Step 1400: Loss = 0.1221, Accuracy = 0.9629\n",
      "Epoch 2 Step 1600: Loss = 0.1842, Accuracy = 0.9627\n",
      "Epoch 2 Step 1800: Loss = 0.1324, Accuracy = 0.9631\n",
      "End of epoch 2, loss: 0.021716831251978874, accuracy: 0.9639166593551636\n",
      "\n",
      "ğŸ”¹ Test Accuracy: 0.9644\n",
      "Epoch 1/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5305 - loss: 0.6908   \n",
      "Epoch 2/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5200 - loss: 0.6887 \n",
      "Epoch 3/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5246 - loss: 0.6871 \n",
      "Epoch 4/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5597 - loss: 0.6818 \n",
      "Epoch 5/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5646 - loss: 0.6796 \n",
      "Epoch 6/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5938 - loss: 0.6768 \n",
      "Epoch 7/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5879 - loss: 0.6785 \n",
      "Epoch 8/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5932 - loss: 0.6775 \n",
      "Epoch 9/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5818 - loss: 0.6791 \n",
      "Epoch 10/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5660 - loss: 0.6720 \n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5175 - loss: 0.6912  \n",
      "\n",
      "ğŸ”¹ Test Loss: 0.6915, Test Accuracy: 0.5100\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize pixel values (0 to 1)\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Create a batched dataset for training\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "\n",
    "# Define a simple neural network\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),  # Flatten the input\n",
    "    Dense(128, activation='relu'),  # First hidden layer\n",
    "    Dense(10)  # Output layer (10 classes for digits 0-9)\n",
    "])\n",
    "# Define loss function for multi-class classification\n",
    "loss_fn = SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Define optimizer (Adam)\n",
    "optimizer = Adam()\n",
    "\n",
    "# Define accuracy metric\n",
    "accuracy_metric = SparseCategoricalAccuracy()\n",
    "# Custom callback to print loss and accuracy after each epoch\n",
    "class CustomCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        print(f'End of epoch {epoch + 1}, loss: {logs.get(\"loss\")}, accuracy: {logs.get(\"accuracy\")}')\n",
    "\n",
    "# Training parameters\n",
    "epochs = 2\n",
    "custom_callback = CustomCallback()\n",
    "\n",
    "# Custom training loop\n",
    "for epoch in range(epochs):\n",
    "    print(f'\\nğŸ”¹ Start of epoch {epoch + 1}')\n",
    "    \n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            loss_value = loss_fn(y_batch_train, logits)  # Compute loss\n",
    "        \n",
    "        # Compute gradients\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))  # Update weights\n",
    "        \n",
    "        # Update accuracy metric\n",
    "        accuracy_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "        # Print loss and accuracy every 200 steps\n",
    "        if step % 200 == 0:\n",
    "            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy():.4f}, Accuracy = {accuracy_metric.result().numpy():.4f}')\n",
    "    \n",
    "    # Call custom callback at the end of each epoch\n",
    "    custom_callback.on_epoch_end(epoch, logs={'loss': loss_value.numpy(), 'accuracy': accuracy_metric.result().numpy()})\n",
    "    \n",
    "    # Reset the metric after each epoch\n",
    "    accuracy_metric.reset_state()\n",
    "\n",
    "# Convert test set to a dataset\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_accuracy = SparseCategoricalAccuracy()\n",
    "for x_batch_test, y_batch_test in test_dataset:\n",
    "    logits = model(x_batch_test, training=False)\n",
    "    test_accuracy.update_state(y_batch_test, logits)\n",
    "\n",
    "print(f\"\\nğŸ”¹ Test Accuracy: {test_accuracy.result().numpy():.4f}\")\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "\n",
    "# Define a simple model for 20-feature input\n",
    "model = Sequential([\n",
    "    Input(shape=(20,)),  # Adjust input shape to (20,)\n",
    "    Dense(128, activation='relu'),  # Hidden layer with 128 neurons\n",
    "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Generate example dataset\n",
    "X_train = np.random.rand(1000, 20)  # 1000 samples, 20 features\n",
    "y_train = np.random.randint(2, size=(1000, 1))  # Binary labels (0 or 1)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Generate test data\n",
    "X_test = np.random.rand(200, 20)  # 200 samples, 20 features\n",
    "y_test = np.random.randint(2, size=(200, 1))  # 200 binary labels\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"\\nğŸ”¹ Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click here for solution</summary> </br>\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Flatten \n",
    "\n",
    "# Step 1: Set Up the Environment\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() \n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32) \n",
    "\n",
    "# Step 2: Define the Model\n",
    "model = Sequential([ \n",
    "    Flatten(input_shape=(28, 28)), \n",
    "    Dense(128, activation='relu'), \n",
    "    Dense(10) \n",
    "]) \n",
    "\n",
    "# Step 3: Define Loss Function and Optimizer\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) \n",
    "optimizer = tf.keras.optimizers.Adam() \n",
    "\n",
    "# Step 4: Implement the Custom Training Loop\n",
    "for epoch in range(5): \n",
    "    for x_batch, y_batch in train_dataset: \n",
    "        with tf.GradientTape() as tape: \n",
    "            logits = model(x_batch, training=True) \n",
    "            loss = loss_fn(y_batch, logits) \n",
    "        grads = tape.gradient(loss, model.trainable_weights) \n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights)) \n",
    "    print(f'Epoch {epoch + 1}: Loss = {loss.numpy()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Adding Accuracy Metric \n",
    "\n",
    "#### Objective: Enhance the custom training loop by adding an accuracy metric to monitor model performance. \n",
    "\n",
    "#### Instructions: \n",
    "\n",
    "1. Set up the environment and define the model, loss function, and optimizer. \n",
    "\n",
    "2. Add Sparse Categorical Accuracy as a metric. \n",
    "\n",
    "3. Implement the custom training loop with accuracy tracking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¹ Start of Epoch 1\n",
      "Epoch 1 Step 0: Loss = 2.4127, Accuracy = 0.0625\n",
      "Epoch 1 Step 200: Loss = 0.3922, Accuracy = 0.8335\n",
      "Epoch 1 Step 400: Loss = 0.1720, Accuracy = 0.8683\n",
      "Epoch 1 Step 600: Loss = 0.1735, Accuracy = 0.8836\n",
      "Epoch 1 Step 800: Loss = 0.1725, Accuracy = 0.8961\n",
      "Epoch 1 Step 1000: Loss = 0.4678, Accuracy = 0.9031\n",
      "Epoch 1 Step 1200: Loss = 0.2063, Accuracy = 0.9098\n",
      "Epoch 1 Step 1400: Loss = 0.2847, Accuracy = 0.9149\n",
      "Epoch 1 Step 1600: Loss = 0.2226, Accuracy = 0.9182\n",
      "Epoch 1 Step 1800: Loss = 0.2097, Accuracy = 0.9222\n",
      "ğŸ”¹ End of Epoch 1: Accuracy = 0.9242\n",
      "\n",
      "ğŸ”¹ Start of Epoch 2\n",
      "Epoch 2 Step 0: Loss = 0.0679, Accuracy = 1.0000\n",
      "Epoch 2 Step 200: Loss = 0.1307, Accuracy = 0.9635\n",
      "Epoch 2 Step 400: Loss = 0.1046, Accuracy = 0.9596\n",
      "Epoch 2 Step 600: Loss = 0.0476, Accuracy = 0.9616\n",
      "Epoch 2 Step 800: Loss = 0.0752, Accuracy = 0.9631\n",
      "Epoch 2 Step 1000: Loss = 0.2943, Accuracy = 0.9634\n",
      "Epoch 2 Step 1200: Loss = 0.0920, Accuracy = 0.9641\n",
      "Epoch 2 Step 1400: Loss = 0.1500, Accuracy = 0.9646\n",
      "Epoch 2 Step 1600: Loss = 0.1911, Accuracy = 0.9647\n",
      "Epoch 2 Step 1800: Loss = 0.1177, Accuracy = 0.9654\n",
      "ğŸ”¹ End of Epoch 2: Accuracy = 0.9660\n",
      "\n",
      "ğŸ”¹ Start of Epoch 3\n",
      "Epoch 3 Step 0: Loss = 0.0387, Accuracy = 1.0000\n",
      "Epoch 3 Step 200: Loss = 0.1189, Accuracy = 0.9759\n",
      "Epoch 3 Step 400: Loss = 0.0872, Accuracy = 0.9737\n",
      "Epoch 3 Step 600: Loss = 0.0312, Accuracy = 0.9750\n",
      "Epoch 3 Step 800: Loss = 0.0605, Accuracy = 0.9755\n",
      "Epoch 3 Step 1000: Loss = 0.1721, Accuracy = 0.9756\n",
      "Epoch 3 Step 1200: Loss = 0.0600, Accuracy = 0.9757\n",
      "Epoch 3 Step 1400: Loss = 0.0712, Accuracy = 0.9760\n",
      "Epoch 3 Step 1600: Loss = 0.1143, Accuracy = 0.9758\n",
      "Epoch 3 Step 1800: Loss = 0.0722, Accuracy = 0.9763\n",
      "ğŸ”¹ End of Epoch 3: Accuracy = 0.9767\n",
      "\n",
      "ğŸ”¹ Start of Epoch 4\n",
      "Epoch 4 Step 0: Loss = 0.0264, Accuracy = 1.0000\n",
      "Epoch 4 Step 200: Loss = 0.0884, Accuracy = 0.9834\n",
      "Epoch 4 Step 400: Loss = 0.0819, Accuracy = 0.9814\n",
      "Epoch 4 Step 600: Loss = 0.0344, Accuracy = 0.9825\n",
      "Epoch 4 Step 800: Loss = 0.0401, Accuracy = 0.9820\n",
      "Epoch 4 Step 1000: Loss = 0.1130, Accuracy = 0.9822\n",
      "Epoch 4 Step 1200: Loss = 0.0376, Accuracy = 0.9821\n",
      "Epoch 4 Step 1400: Loss = 0.0429, Accuracy = 0.9826\n",
      "Epoch 4 Step 1600: Loss = 0.0603, Accuracy = 0.9825\n",
      "Epoch 4 Step 1800: Loss = 0.0417, Accuracy = 0.9828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 14:15:02.269183: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹ End of Epoch 4: Accuracy = 0.9831\n",
      "\n",
      "ğŸ”¹ Start of Epoch 5\n",
      "Epoch 5 Step 0: Loss = 0.0197, Accuracy = 1.0000\n",
      "Epoch 5 Step 200: Loss = 0.0521, Accuracy = 0.9879\n",
      "Epoch 5 Step 400: Loss = 0.0685, Accuracy = 0.9871\n",
      "Epoch 5 Step 600: Loss = 0.0244, Accuracy = 0.9874\n",
      "Epoch 5 Step 800: Loss = 0.0380, Accuracy = 0.9871\n",
      "Epoch 5 Step 1000: Loss = 0.0765, Accuracy = 0.9872\n",
      "Epoch 5 Step 1200: Loss = 0.0315, Accuracy = 0.9871\n",
      "Epoch 5 Step 1400: Loss = 0.0329, Accuracy = 0.9872\n",
      "Epoch 5 Step 1600: Loss = 0.0330, Accuracy = 0.9872\n",
      "Epoch 5 Step 1800: Loss = 0.0208, Accuracy = 0.9875\n",
      "ğŸ”¹ End of Epoch 5: Accuracy = 0.9878\n",
      "\n",
      "ğŸ”¹ Final Test Accuracy: 0.9752\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize pixel values (0 to 1)\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Create a batched dataset\n",
    "batch_size = 32\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
    "\n",
    "# Define a simple neural network\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),  # Flatten 28x28 images\n",
    "    Dense(128, activation='relu'),  # Hidden layer with 128 neurons\n",
    "    Dense(10)  # Output layer with 10 neurons (for digits 0-9)\n",
    "])\n",
    "\n",
    "# Loss function for multi-class classification\n",
    "loss_fn = SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = Adam()\n",
    "\n",
    "# Accuracy metric to track training progress\n",
    "train_accuracy_metric = SparseCategoricalAccuracy()\n",
    "test_accuracy_metric = SparseCategoricalAccuracy()\n",
    "\n",
    "# Training parameters\n",
    "epochs = 5\n",
    "\n",
    "# Custom training loop\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nğŸ”¹ Start of Epoch {epoch + 1}\")\n",
    "\n",
    "    # Reset training accuracy at the start of each epoch\n",
    "    train_accuracy_metric.reset_state()\n",
    "\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass: Compute predictions\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            # Compute loss\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "        \n",
    "        # Compute gradients\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        # Apply gradients (update weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        # Update accuracy metric\n",
    "        train_accuracy_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "        # Print loss and accuracy every 200 steps\n",
    "        if step % 200 == 0:\n",
    "            print(f\"Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy():.4f}, Accuracy = {train_accuracy_metric.result().numpy():.4f}\")\n",
    "    \n",
    "    # Print accuracy at the end of each epoch\n",
    "    print(f\"ğŸ”¹ End of Epoch {epoch + 1}: Accuracy = {train_accuracy_metric.result().numpy():.4f}\")\n",
    "\n",
    "# Reset test accuracy\n",
    "test_accuracy_metric.reset_state()\n",
    "\n",
    "# Evaluate on test data\n",
    "for x_batch_test, y_batch_test in test_dataset:\n",
    "    logits = model(x_batch_test, training=False)  # No training\n",
    "    test_accuracy_metric.update_state(y_batch_test, logits)\n",
    "\n",
    "# Print final test accuracy\n",
    "print(f\"\\nğŸ”¹ Final Test Accuracy: {test_accuracy_metric.result().numpy():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click here for solution</summary><br>\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Flatten \n",
    "\n",
    "# Step 1: Set Up the Environment\n",
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data() \n",
    "x_train = x_train / 255.0 \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32) \n",
    "\n",
    "# Step 2: Define the Model\n",
    "model = Sequential([ \n",
    "    Flatten(input_shape=(28, 28)), \n",
    "    Dense(128, activation='relu'), \n",
    "    Dense(10) \n",
    "]) \n",
    "\n",
    "# Step 3: Define Loss Function, Optimizer, and Metric\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) \n",
    "optimizer = tf.keras.optimizers.Adam() \n",
    "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy() \n",
    "\n",
    "# Step 4: Implement the Custom Training Loop with Accuracy Tracking\n",
    "epochs = 5 \n",
    "for epoch in range(epochs): \n",
    "    for x_batch, y_batch in train_dataset: \n",
    "        with tf.GradientTape() as tape: \n",
    "            logits = model(x_batch, training=True) \n",
    "            loss = loss_fn(y_batch, logits) \n",
    "        grads = tape.gradient(loss, model.trainable_weights) \n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights)) \n",
    "        accuracy_metric.update_state(y_batch, logits) \n",
    "    print(f'Epoch {epoch + 1}: Loss = {loss.numpy()} Accuracy = {accuracy_metric.result().numpy()}') \n",
    "    accuracy_metric.reset_state() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Custom Callback for Advanced Logging \n",
    "\n",
    "#### Objective: Implement a custom callback to log additional metrics and information during training. \n",
    "\n",
    "#### Instructions: \n",
    "\n",
    "1. Set up the environment and define the model, loss function, optimizer, and metric. \n",
    "\n",
    "2. Create a custom callback to log additional metrics at the end of each epoch. \n",
    "\n",
    "3. Implement the custom training loop with the custom callback. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¹ Start of Epoch 1\n",
      "Epoch 1 Step 0: Loss = 2.2870, Accuracy = 0.1250\n",
      "Epoch 1 Step 200: Loss = 0.4030, Accuracy = 0.8318\n",
      "Epoch 1 Step 400: Loss = 0.1706, Accuracy = 0.8660\n",
      "Epoch 1 Step 600: Loss = 0.1510, Accuracy = 0.8815\n",
      "Epoch 1 Step 800: Loss = 0.1579, Accuracy = 0.8944\n",
      "Epoch 1 Step 1000: Loss = 0.4030, Accuracy = 0.9011\n",
      "Epoch 1 Step 1200: Loss = 0.1572, Accuracy = 0.9077\n",
      "Epoch 1 Step 1400: Loss = 0.3048, Accuracy = 0.9129\n",
      "Epoch 1 Step 1600: Loss = 0.2445, Accuracy = 0.9163\n",
      "Epoch 1 Step 1800: Loss = 0.1328, Accuracy = 0.9204\n",
      "ğŸ”¹ End of Epoch 1: Loss = 0.0464, Accuracy = 0.9224\n",
      "\n",
      "ğŸ”¹ Start of Epoch 2\n",
      "Epoch 2 Step 0: Loss = 0.0837, Accuracy = 1.0000\n",
      "Epoch 2 Step 200: Loss = 0.1586, Accuracy = 0.9599\n",
      "Epoch 2 Step 400: Loss = 0.1048, Accuracy = 0.9575\n",
      "Epoch 2 Step 600: Loss = 0.0430, Accuracy = 0.9591\n",
      "Epoch 2 Step 800: Loss = 0.0961, Accuracy = 0.9604\n",
      "Epoch 2 Step 1000: Loss = 0.3571, Accuracy = 0.9604\n",
      "Epoch 2 Step 1200: Loss = 0.1035, Accuracy = 0.9612\n",
      "Epoch 2 Step 1400: Loss = 0.2455, Accuracy = 0.9619\n",
      "Epoch 2 Step 1600: Loss = 0.1867, Accuracy = 0.9621\n",
      "Epoch 2 Step 1800: Loss = 0.0881, Accuracy = 0.9632\n",
      "ğŸ”¹ End of Epoch 2: Loss = 0.0549, Accuracy = 0.9638\n",
      "\n",
      "ğŸ”¹ Start of Epoch 3\n",
      "Epoch 3 Step 0: Loss = 0.0385, Accuracy = 1.0000\n",
      "Epoch 3 Step 200: Loss = 0.0824, Accuracy = 0.9751\n",
      "Epoch 3 Step 400: Loss = 0.0801, Accuracy = 0.9730\n",
      "Epoch 3 Step 600: Loss = 0.0259, Accuracy = 0.9734\n",
      "Epoch 3 Step 800: Loss = 0.0612, Accuracy = 0.9737\n",
      "Epoch 3 Step 1000: Loss = 0.2486, Accuracy = 0.9738\n",
      "Epoch 3 Step 1200: Loss = 0.0491, Accuracy = 0.9740\n",
      "Epoch 3 Step 1400: Loss = 0.1275, Accuracy = 0.9745\n",
      "Epoch 3 Step 1600: Loss = 0.1126, Accuracy = 0.9742\n",
      "Epoch 3 Step 1800: Loss = 0.0613, Accuracy = 0.9746\n",
      "ğŸ”¹ End of Epoch 3: Loss = 0.0447, Accuracy = 0.9749\n",
      "\n",
      "ğŸ”¹ Start of Epoch 4\n",
      "Epoch 4 Step 0: Loss = 0.0240, Accuracy = 1.0000\n",
      "Epoch 4 Step 200: Loss = 0.0876, Accuracy = 0.9831\n",
      "Epoch 4 Step 400: Loss = 0.0550, Accuracy = 0.9810\n",
      "Epoch 4 Step 600: Loss = 0.0287, Accuracy = 0.9813\n",
      "Epoch 4 Step 800: Loss = 0.0269, Accuracy = 0.9813\n",
      "Epoch 4 Step 1000: Loss = 0.1596, Accuracy = 0.9815\n",
      "Epoch 4 Step 1200: Loss = 0.0238, Accuracy = 0.9813\n",
      "Epoch 4 Step 1400: Loss = 0.0674, Accuracy = 0.9817\n",
      "Epoch 4 Step 1600: Loss = 0.0553, Accuracy = 0.9815\n",
      "Epoch 4 Step 1800: Loss = 0.0310, Accuracy = 0.9817\n",
      "ğŸ”¹ End of Epoch 4: Loss = 0.0355, Accuracy = 0.9820\n",
      "\n",
      "ğŸ”¹ Start of Epoch 5\n",
      "Epoch 5 Step 0: Loss = 0.0188, Accuracy = 1.0000\n",
      "Epoch 5 Step 200: Loss = 0.0500, Accuracy = 0.9863\n",
      "Epoch 5 Step 400: Loss = 0.0379, Accuracy = 0.9861\n",
      "Epoch 5 Step 600: Loss = 0.0392, Accuracy = 0.9867\n",
      "Epoch 5 Step 800: Loss = 0.0178, Accuracy = 0.9869\n",
      "Epoch 5 Step 1000: Loss = 0.1550, Accuracy = 0.9866\n",
      "Epoch 5 Step 1200: Loss = 0.0110, Accuracy = 0.9865\n",
      "Epoch 5 Step 1400: Loss = 0.0222, Accuracy = 0.9866\n",
      "Epoch 5 Step 1600: Loss = 0.0503, Accuracy = 0.9864\n",
      "Epoch 5 Step 1800: Loss = 0.0252, Accuracy = 0.9866\n",
      "ğŸ”¹ End of Epoch 5: Loss = 0.0297, Accuracy = 0.9868\n",
      "\n",
      "ğŸ”¹ Final Test Accuracy: 0.9713\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 101\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Print final test accuracy\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mğŸ”¹ Final Test Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_accuracy_metric\u001b[38;5;241m.\u001b[39mresult()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Plot loss over epochs\u001b[39;00m\n\u001b[1;32m    104\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize pixel values (0 to 1)\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Create a batched dataset\n",
    "batch_size = 32\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
    "\n",
    "# Define a simple neural network\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),  # Flatten 28x28 images\n",
    "    Dense(128, activation='relu'),  # Hidden layer with 128 neurons\n",
    "    Dense(10)  # Output layer with 10 neurons (for digits 0-9)\n",
    "])\n",
    "\n",
    "# Loss function for multi-class classification\n",
    "loss_fn = SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = Adam()\n",
    "\n",
    "# Accuracy metric to track training progress\n",
    "train_accuracy_metric = SparseCategoricalAccuracy()\n",
    "test_accuracy_metric = SparseCategoricalAccuracy()\n",
    "\n",
    "# Custom callback to log additional metrics\n",
    "class CustomLoggingCallback(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.epoch_losses = []\n",
    "        self.epoch_accuracies = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        loss = logs.get(\"loss\", 0)\n",
    "        accuracy = logs.get(\"accuracy\", 0)\n",
    "        \n",
    "        self.epoch_losses.append(loss)\n",
    "        self.epoch_accuracies.append(accuracy)\n",
    "        \n",
    "        print(f\"ğŸ”¹ End of Epoch {epoch + 1}: Loss = {loss:.4f}, Accuracy = {accuracy:.4f}\")\n",
    "\n",
    "# Initialize callback\n",
    "custom_callback = CustomLoggingCallback()\n",
    "\n",
    "# Training parameters\n",
    "epochs = 5\n",
    "\n",
    "# Custom training loop\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nğŸ”¹ Start of Epoch {epoch + 1}\")\n",
    "\n",
    "    # Reset training accuracy at the start of each epoch\n",
    "    train_accuracy_metric.reset_state()\n",
    "\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass: Compute predictions\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            # Compute loss\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "        \n",
    "        # Compute gradients\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        # Apply gradients (update weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        # Update accuracy metric\n",
    "        train_accuracy_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "        # Print loss and accuracy every 200 steps\n",
    "        if step % 200 == 0:\n",
    "            print(f\"Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy():.4f}, Accuracy = {train_accuracy_metric.result().numpy():.4f}\")\n",
    "    \n",
    "    # Call custom callback at the end of each epoch\n",
    "    custom_callback.on_epoch_end(epoch, logs={'loss': loss_value.numpy(), 'accuracy': train_accuracy_metric.result().numpy()})\n",
    "\n",
    "# Reset test accuracy\n",
    "test_accuracy_metric.reset_state()\n",
    "\n",
    "# Evaluate on test data\n",
    "for x_batch_test, y_batch_test in test_dataset:\n",
    "    logits = model(x_batch_test, training=False)  # No training\n",
    "    test_accuracy_metric.update_state(y_batch_test, logits)\n",
    "\n",
    "# Print final test accuracy\n",
    "print(f\"\\nğŸ”¹ Final Test Accuracy: {test_accuracy_metric.result().numpy():.4f}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot loss over epochs\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, epochs + 1), custom_callback.epoch_losses, label=\"Loss\", marker='o')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Over Epochs\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot accuracy over epochs\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, epochs + 1), custom_callback.epoch_accuracies, label=\"Accuracy\", marker='o')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training Accuracy Over Epochs\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1cAAAHWCAYAAACbsXOkAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAdihJREFUeJzt3XlYVGX/BvB7ZmBm2PdtEBHQREEWUclyyVxwozQrNU2z3sxdX39lmr5uvWWraVpqplaiaeZSWGKGqa9mWmwuqKkoIDsouwwwc35/KJMjiwwCh+X+XNe5LnnOc858z+Fkc/uc8xyJIAgCiIiIiIiI6KFIxS6AiIiIiIioJWC4IiIiIiIiqgcMV0RERERERPWA4YqIiIiIiKgeMFwRERERERHVA4YrIiIiIiKiesBwRUREREREVA8YroiIiIiIiOoBwxUREREREVE9YLgiImoCXnrpJbRr165O2y5duhQSiaR+CyJqxo4cOQKJRILvv/9e7FKIqJVhuCIiqoFEIqnVcuTIEbFLFd3zzz8PiUSCN998U+xSmqUTJ05g5MiRcHJygkKhQLt27fDaa68hKSlJ7NIqqQgv1S07duwQu0QiIlFIBEEQxC6CiKipCgsL0/v5m2++waFDh7B161a99oEDB8LJyanOn1NWVgatVguFQmHwtuXl5SgvL4dSqazz5z+s/Px8ODk5wdnZGRqNBomJiRxNM8CaNWswe/ZseHp64qWXXoKLiwsuXLiAL7/8EgDw888/47HHHhO5yn8cOXIE/fr1w6xZs9C9e/dK63v37g13d3cRKrujor5du3bh2WefFa0OImp9jMQugIioKRs/frzez3/88QcOHTpUqf1+xcXFMDU1rfXnGBsb16k+ADAyMoKRkbh/ne/evRsajQabN2/Gk08+iWPHjqFv376i1lQVQRBQUlICExMTsUvROXHiBObMmYNevXohIiJC77qZOnUqHn/8cTz77LM4f/48bGxsGq2uoqIimJmZ1dind+/eDC9ERPfgbYFERA/piSeegK+vL6KiotCnTx+YmprirbfeAgD88MMPGDZsGFQqFRQKBby8vPD2229Do9Ho7eP+Z66uX78OiUSCjz76CF988QW8vLygUCjQvXt3/Pnnn3rbVvXMlUQiwYwZM7Bv3z74+vpCoVDAx8cHERERleo/cuQIunXrBqVSCS8vL2zYsMHg57i2bduGgQMHol+/fujUqRO2bdtWZb+LFy/i+eefh4ODA0xMTNCxY0csXLhQr09KSgpeeeUV3Tnz8PDA1KlTUVpaWu3xAsBXX30FiUSC69ev69ratWuH4cOH4+DBg+jWrRtMTEywYcMGAMCWLVvw5JNPwtHREQqFAp07d8a6deuqrPvAgQPo27cvLCwsYGlpie7du2P79u0AgCVLlsDY2BhZWVmVtps8eTKsra1RUlJS7bl7++23IZFI8PXXX1cK5F5eXvjggw+Qlpamq/ujjz6CRCJBYmJipX0tWLAAcrkct27d0rWdOnUKgwcPhpWVFUxNTdG3b1+cOHFCb7uKcxofH48XXngBNjY26NWrV7U1G6LiWty2bRs6duwIpVKJoKAgHDt2rFLfmJgYDBkyBJaWljA3N0f//v3xxx9/VOqXm5uLf//732jXrh0UCgXatGmDCRMmIDs7W6+fVqvFO++8gzZt2kCpVKJ///64cuWKXp/Lly9j1KhRcHZ2hlKpRJs2bTBmzBjk5eXVy/ETUevCkSsionqQk5ODIUOGYMyYMRg/frzuFsGvvvoK5ubmmDt3LszNzXH48GEsXrwY+fn5+PDDDx+43+3bt6OgoACvvfYaJBIJPvjgAzzzzDNISEh44GjX8ePHsWfPHkybNg0WFhb49NNPMWrUKCQlJcHOzg7AnS+zgwcPhouLC5YtWwaNRoPly5fDwcGh1seempqK3377DV9//TUAYOzYsfjkk0+wdu1ayOVyXb8zZ86gd+/eMDY2xuTJk9GuXTtcvXoV4eHheOedd3T76tGjB3JzczF58mR4e3sjJSUF33//PYqLi/X2V1uXLl3C2LFj8dprr+HVV19Fx44dAQDr1q2Dj48PnnrqKRgZGSE8PBzTpk2DVqvF9OnTddt/9dVXePnll+Hj44MFCxbA2toaMTExiIiIwAsvvIAXX3wRy5cvx86dOzFjxgzddqWlpfj+++8xatSoam/ZLC4uRmRkJHr37g0PD48q+4wePRqTJ0/G/v37MX/+fDz//POYN28evvvuO7zxxht6fb/77jsMGjRIN8J1+PBhDBkyBEFBQViyZAmkUqkuVP7vf/9Djx499LZ/7rnn0KFDB7z77ruozVMDBQUFlQINANjZ2ekF4KNHj2Lnzp2YNWsWFAoFPv/8cwwePBinT5+Gr68vAOD8+fPo3bs3LC0tMW/ePBgbG2PDhg144okncPToUQQHBwMACgsL0bt3b1y4cAEvv/wyunbtiuzsbPz444+4ceMG7O3tdZ/73nvvQSqV4vXXX0deXh4++OADjBs3DqdOnQJw53cUEhICtVqNmTNnwtnZGSkpKdi/fz9yc3NhZWX1wHNARKRHICKiWps+fbpw/1+dffv2FQAI69evr9S/uLi4Uttrr70mmJqaCiUlJbq2iRMnCu7u7rqfr127JgAQ7OzshJs3b+raf/jhBwGAEB4ermtbsmRJpZoACHK5XLhy5YquLS4uTgAgrFmzRtcWGhoqmJqaCikpKbq2y5cvC0ZGRpX2WZ2PPvpIMDExEfLz8wVBEIS///5bACDs3btXr1+fPn0ECwsLITExUa9dq9Xq/jxhwgRBKpUKf/75Z6XPqehX1fEKgiBs2bJFACBcu3ZN1+bu7i4AECIiIir1r+p3ExISInh6eup+zs3NFSwsLITg4GDh9u3b1dbds2dPITg4WG/9nj17BADCb7/9VulzKsTGxgoAhNmzZ1fbRxAEwc/PT7C1tdX7vKCgIL0+p0+fFgAI33zzja6+Dh06CCEhIXq1FhcXCx4eHsLAgQN1bRXndOzYsTXWUeG3334TAFS7pKWl6fpWtP3111+6tsTEREGpVAojR47UtY0YMUKQy+XC1atXdW2pqamChYWF0KdPH13b4sWLBQDCnj17KtVVcZwV9XXq1ElQq9W69atXrxYACGfPnhUEQRBiYmIEAMKuXbtqddxERA/C2wKJiOqBQqHApEmTKrXf+2xPxb/y9+7dG8XFxbh48eID9zt69Gi952x69+4NAEhISHjgtgMGDICXl5fuZz8/P1haWuq21Wg0+PXXXzFixAioVCpdv/bt22PIkCEP3H+Fbdu2YdiwYbCwsAAAdOjQAUFBQXq3BmZlZeHYsWN4+eWX0bZtW73tK0Y4tFot9u3bh9DQUHTr1q3S59R1ggwPDw+EhIRUar/3d5OXl4fs7Gz07dsXCQkJulvCDh06hIKCAsyfP7/S6NO99UyYMAGnTp3C1atXdW3btm2Dm5tbjc+eFRQUAIDu3FXHwsIC+fn5up9Hjx6NqKgovc/buXMnFAoFnn76aQBAbGwsLl++jBdeeAE5OTnIzs5GdnY2ioqK0L9/fxw7dgxarVbvc6ZMmVJjHfdbvHgxDh06VGmxtbXV69ezZ08EBQXpfm7bti2efvppHDx4EBqNBhqNBr/88gtGjBgBT09PXT8XFxe88MILOH78uO74d+/eDX9/f4wcObJSPfdfI5MmTdIb7bz/v5+KkamDBw+iuLjYoGMnIqoKwxURUT1wdXWt8pa18+fPY+TIkbCysoKlpSUcHBx0k2HU5pmO+4NIRdC695ma2m5bsX3FtpmZmbh9+zbat29fqV9VbVW5cOECYmJi8Pjjj+PKlSu65YknnsD+/ft1X4grvsxW3AJWlaysLOTn59fYpy6qu93uxIkTGDBgAMzMzGBtbQ0HBwfds3IVv5uK8PKgmkaPHg2FQqELlHl5edi/fz/GjRtXYyisCFUVIas6BQUFegHsueeeg1Qqxc6dOwHcmahj165duueVgDvPEgHAxIkT4eDgoLd8+eWXUKvVla7B6s5Vdbp06YIBAwZUWu7/b6FDhw6Vtn3kkUdQXFyMrKwsZGVlobi4WHfL5r06deoErVaL5ORkAHd+J7W9Rh7034+Hhwfmzp2LL7/8Evb29ggJCcFnn33G562IqM4YroiI6kFVs8/l5uaib9++iIuLw/LlyxEeHo5Dhw7h/fffB4BKowZVkclkVbYLtXge5mG2ra2Kqer//e9/o0OHDrrl448/RklJCXbv3l1vn1WhurBy/yQhFar63Vy9ehX9+/dHdnY2Vq5ciZ9++gmHDh3Cv//9bwC1+93cy8bGBsOHD9eFq++//x5qtfqBs0q2b98eRkZGOHPmTLV91Go1Ll26hM6dO+vaVCoVevfuje+++w7AnVksk5KSMHr0aF2fimP48MMPqxxdOnToEMzNzfU+qynNolgfavPfwMcff4wzZ87grbfewu3btzFr1iz4+Pjgxo0bjVUmEbUgnNCCiKiBHDlyBDk5OdizZw/69Omja7927ZqIVf3D0dERSqWy0uxpAKpsu58gCNi+fTv69euHadOmVVr/9ttvY9u2bZg0aZLuVq9z585Vuz8HBwdYWlrW2Af4Z/QhNzcX1tbWuvaqZs+rTnh4ONRqNX788Ue90Y3ffvtNr1/FbZXnzp174GjehAkT8PTTT+PPP//Etm3bEBgYCB8fnxq3MTMzQ79+/XD48GEkJiZW+W6o7777Dmq1GsOHD9drHz16NKZNm4ZLly5h586dMDU1RWhoaKXaLS0tMWDAgBrraGgVo2j3+vvvv2FqaqqbPMXU1BSXLl2q1O/ixYuQSqVwc3MDcOe4HnSNGKpLly7o0qULFi1ahN9//x2PP/441q9fj//+97/1+jlE1PJx5IqIqIFU/Kv5vf9KXlpais8//1yskvTIZDIMGDAA+/btQ2pqqq79ypUrOHDgwAO3P3HiBK5fv45Jkybh2WefrbSMHj0av/32G1JTU+Hg4IA+ffpg8+bNSEpK0ttPxfmRSqUYMWIEwsPD8ddff1X6vIp+FaHh3qm8i4qKdLMV1vbY790ncOdWvi1btuj1GzRoECwsLLBixYpK06nfPwI4ZMgQ2Nvb4/3338fRo0cfOGpVYdGiRRAEAS+99BJu376tt+7atWuYN28eXFxc8Nprr+mtGzVqFGQyGb799lvs2rULw4cP13svVVBQELy8vPDRRx+hsLCw0udWNXV8Qzl58iSio6N1PycnJ+OHH37AoEGDIJPJIJPJMGjQIPzwww96U+lnZGRg+/bt6NWrl+52x1GjRiEuLg579+6t9DmGjsrm5+ejvLxcr61Lly6QSqVQq9UG7YuICODIFRFRg3nsscdgY2ODiRMnYtasWZBIJNi6dWu93pb3sJYuXYpffvkFjz/+OKZOnQqNRoO1a9fC19cXsbGxNW67bds2yGQyDBs2rMr1Tz31FBYuXIgdO3Zg7ty5+PTTT9GrVy907doVkydPhoeHB65fv46ffvpJ91nvvvsufvnlF/Tt2xeTJ09Gp06dkJaWhl27duH48eOwtrbGoEGD0LZtW7zyyit44403IJPJsHnzZjg4OFQKbtUZNGgQ5HI5QkND8dprr6GwsBAbN26Eo6Mj0tLSdP0sLS3xySef4F//+he6d++uewdUXFwciouL9QKdsbExxowZg7Vr10Imk2Hs2LG1qqVPnz746KOPMHfuXPj5+eGll16Ci4sLLl68iI0bN0Kr1eLnn3+u9AJhR0dH9OvXDytXrkRBQYHeLYHAnbD65ZdfYsiQIfDx8cGkSZPg6uqKlJQU/Pbbb7C0tER4eHitaqzO//73vyrf4eXn5wc/Pz/dz76+vggJCdGbih0Ali1bpuvz3//+F4cOHUKvXr0wbdo0GBkZYcOGDVCr1fjggw90/d544w18//33eO655/Dyyy8jKCgIN2/exI8//oj169fD39+/1vUfPnwYM2bMwHPPPYdHHnkE5eXl2Lp1K2QyGUaNGlWXU0JErZ0ocxQSETVT1U3F7uPjU2X/EydOCI8++qhgYmIiqFQqYd68ecLBgwcrTdFd3VTsH374YaV9AhCWLFmi+7m6qdinT59eaVt3d3dh4sSJem2RkZFCYGCgIJfLBS8vL+HLL78U/u///k9QKpXVnAVBKC0tFezs7ITevXtX20cQBMHDw0MIDAzU/Xzu3Dlh5MiRgrW1taBUKoWOHTsK//nPf/S2SUxMFCZMmCA4ODgICoVC8PT0FKZPn643pXZUVJQQHBwsyOVyoW3btsLKlSurnYp92LBhVdb2448/Cn5+foJSqRTatWsnvP/++8LmzZsr7aOi72OPPSaYmJgIlpaWQo8ePYRvv/220j4rpkMfNGhQjeelKseOHROefvppwd7eXjA2Nhbatm0rvPrqq8L169er3Wbjxo0CAMHCwqLSVPEVYmJihGeeeUaws7MTFAqF4O7uLjz//PNCZGSkrk/FNZSVlVWrWh80Ffu912fFtRgWFiZ06NBBUCgUQmBgYJVT1EdHRwshISGCubm5YGpqKvTr10/4/fffK/XLyckRZsyYIbi6ugpyuVxo06aNMHHiRCE7O1uvvvunWK/472rLli2CIAhCQkKC8PLLLwteXl6CUqkUbG1thX79+gm//vprrc4DEdH9JILQhP4JlYiImoQRI0bg/PnzVT4rQ9WLi4tDQEAAvvnmG7z44otil9MkSCQSTJ8+HWvXrhW7FCKiBsdnroiIWrn7n/O5fPkyfv75ZzzxxBPiFNSMbdy4Eebm5njmmWfELoWIiETAZ66IiFo5T09PvPTSS/D09ERiYiLWrVsHuVyOefPmiV1asxEeHo74+Hh88cUXmDFjht7EEkRE1HowXBERtXKDBw/Gt99+i/T0dCgUCvTs2RPvvvtulS9+parNnDkTGRkZGDp0qN4kDURE1LrwmSsiIiIiIqJ6wGeuiIiIiIiI6gHDFRERERERUT3gM1dV0Gq1SE1NhYWFBSQSidjlEBERERGRSARBQEFBAVQqFaTSmsemGK6qkJqaCjc3N7HLICIiIiKiJiI5ORlt2rSpsQ/DVRUsLCwA3DmBlpaWIldDRERERERiyc/Ph5ubmy4j1IThqgoVtwJaWloyXBERERERUa0eF+KEFkRERERERPWA4YqIiIiIiKgeMFwRERERERHVAz5zVUeCIKC8vBwajUbsUshAMpkMRkZGnGafiIiIiOoVw1UdlJaWIi0tDcXFxWKXQnVkamoKFxcXyOVysUshIiIiohaC4cpAWq0W165dg0wmg0qlglwu5whIMyIIAkpLS5GVlYVr166hQ4cOD3wZHBERERFRbTBcGai0tBRarRZubm4wNTUVuxyqAxMTExgbGyMxMRGlpaVQKpVil0RERERELQD/yb6OONrRvPH3R0RERET1jd8wiYiIiIiI6gFvCyQiIiIioiZDoxVw+tpNZBaUwNFCiR4etpBJm8ccBwxXImrOFw4RERERUX2LOJeGZeHxSMsr0bW5WCmxJLQzBvu6iFhZ7TBciUTMC+fkyZPo1asXBg8ejJ9++qlBP4uIiIiIqDYizqVhalg0hPva0/NKMDUsGuvGd23yAYvPXImg4sK5N1gB/1w4EefSGvTzN23ahJkzZ+LYsWNITU1t0M+qSWlpqWifTURERERNh0YrYFl4fKVgBUDXtiw8HhptVT2aDoareiAIAopLy2u1FJSUYcmP52u8cJb+GI+CkrIH7ksQDL+4CgsLsXPnTkydOhXDhg3DV199pbc+PDwc3bt3h1KphL29PUaOHKlbp1ar8eabb8LNzQ0KhQLt27fHpk2bAABfffUVrK2t9fa1b98+vXeALV26FAEBAfjyyy/h4eGhmwI9IiICvXr1grW1Nezs7DB8+HBcvXpVb183btzA2LFjYWtrCzMzM3Tr1g2nTp3C9evXIZVK8ddff+n1X7VqFdzd3aHVag0+R0RERETUeG6XavB9VHKlgYd7CQDS8kpw+trNxiusDnhbYD24XaZB58UH62VfAoD0/BJ0WfrLA/vGLw+BqdywX+F3330Hb29vdOzYEePHj8ecOXOwYMECSCQS/PTTTxg5ciQWLlyIb775BqWlpfj55591206YMAEnT57Ep59+Cn9/f1y7dg3Z2dkGff6VK1ewe/du7NmzBzKZDABQVFSEuXPnws/PD4WFhVi8eDFGjhyJ2NhYSKVSFBYWom/fvnB1dcWPP/4IZ2dnREdHQ6vVol27dhgwYAC2bNmCbt266T5ny5YteOmllzjlOhEREVETUKbRIvlmMa7nFCEhqwjXsu8s17OLkFpDqLpfZkHt+4qB4aqV2bRpE8aPHw8AGDx4MPLy8nD06FE88cQTeOeddzBmzBgsW7ZM19/f3x8A8Pfff+O7777DoUOHMGDAAACAp6enwZ9fWlqKb775Bg4ODrq2UaNG6fXZvHkzHBwcEB8fD19fX2zfvh1ZWVn4888/YWtrCwBo3769rv+//vUvTJkyBStXroRCoUB0dDTOnj2LH374weD6iIiIiKhutFoB6fkluuB075J0s7jGW/rM5DIUlWoe+BmOFsr6LLneMVzVAxNjGeKXh9Sq7+lrN/HSlj8f2O+rSd3Rw8P2gZ9riEuXLuH06dPYu3cvAMDIyAijR4/Gpk2b8MQTTyA2NhavvvpqldvGxsZCJpOhb9++Bn3m/dzd3fWCFQBcvnwZixcvxqlTp5Cdna27lS8pKQm+vr6IjY1FYGCgLljdb8SIEZg+fTr27t2LMWPG4KuvvkK/fv3Qrl27h6qViIiIiPQJgoBbxWW4ll2oG4GqGI26nlOEkrLqH8lQGkvhYW8OD3tTeNib3f2zGTztzWBpYoxe7x9Gel5JlY/PSAA4Wykf+P1YbAxX9UAikdT69rzeHRzgYqV84IXTu4NDvU/LvmnTJpSXl0OlUunaBEGAQqHA2rVrYWJiUu22Na0DAKlUWukZsLKyskr9zMzMKrWFhobC3d0dGzduhEqlglarha+vr27Ciwd9tlwux4QJE7BlyxY888wz2L59O1avXl3jNkRERERUvSJ1eaXRp4S7t/Hl3a78Ha+CkVSCtrYV4ckM7e6GJw8HMzhZKCGt4fvtktDOmBoWDQmg9z1Zcs/6pv7aIoarRiaTSkS5cMrLy/HNN9/g448/xqBBg/TWjRgxAt9++y38/PwQGRmJSZMmVdq+S5cu0Gq1OHr0qO62wHs5ODigoKAARUVFugAVGxv7wLpycnJw6dIlbNy4Eb179wYAHD9+XK+Pn58fvvzyS9y8ebPa0at//etf8PX1xeeff47y8nI888wzD/xsIiIiotZMXa5B8s1i3ajTtex/nofKLFDXuK3KSgkPBzPdCJTn3SDVxsYExrK6PfM+2NcF68Z3rfS6Ime+54pqIsaFs3//fty6dQuvvPIKrKys9NaNGjUKmzZtwocffoj+/fvDy8sLY8aMQXl5OX7++We8+eabaNeuHSZOnIiXX35ZN6FFYmIiMjMz8fzzzyM4OBimpqZ46623MGvWLJw6darSTIRVsbGxgZ2dHb744gu4uLggKSkJ8+fP1+szduxYvPvuuxgxYgRWrFgBFxcXxMTEQKVSoWfPngCATp064dFHH8Wbb76Jl19++YGjXUREREStgUYrIDX3dpXPQd24VYyaZja3M5PrRp887hmBcrc1g4ncsMdTamuwrwsGdnbG6Ws3kVlQAkeLO7cCNvURqwoMVyJp7Atn06ZNGDBgQKVgBdwJVx988AFsbW2xa9cuvP3223jvvfdgaWmJPn366PqtW7cOb731FqZNm4acnBy0bdsWb731FgDA1tYWYWFheOONN7Bx40b0798fS5cuxeTJk2usSyqVYseOHZg1axZ8fX3RsWNHfPrpp3jiiSd0feRyOX755Rf83//9H4YOHYry8nJ07twZn332md6+XnnlFfz+++94+eWXH+JMERERETUvgiAgu7D0bmgqREJ2Ea7dHYFKvFmM0vLqn4Myk8vujkCZ3x2FMr3zZzszWJkaN+JR/EMmlaCnl50on/2wJEJdXpbUwuXn58PKygp5eXmwtLTUW1dSUoJr167pvaeJmoa3334bu3btwpkzZx7Yl79HIiIiam7yS8p0oen+pVBdXu12cpkUbe1M/xl9umdxsFDovZeUKqspG9yPI1fU7BUWFuL69etYu3Yt/vvf/4pdDhEREVGdlZRpkJhTrBuBun5PgMouLK12O4kEaGNjcnfU6e6EEg53noVSWZs0m9vqmjuGK2r2ZsyYgW+//RYjRozgLYFERETU5JVrtEjJva13+17Fkpp3GzXdV+ZgodAbgaqYjc/N1hRKA1/TQ/WP4Yqava+++qpWk2cQERERNRZBEJCRr0ZCdiGuZ98ZiaqYzjz5ZjHKNNUnKAul0T2375nfeSbKzgzt7E1hoRTnOSiqHYYrIiIiIqI6yi0u1R+Byrnz5+s5RSgu1VS7ncJIqvcuqHtHo2zN5HwOqpliuKojzgPSvPH3R0RERLVVXFp+d/Tpntn47j4Pdau4+hfqyqQSuNmY/DMCVTETn4MZXCxrfqEuNU8MVwYyNr4zFFtcXMx3KTVjxcXFAP75fRIREVHrVlquRfKtYt2o072jUen5JTVu62ypvDuBhP6zUG42ppAb1e2FutQ8MVwZSCaTwdraGpmZmQAAU1NTDts2I4IgoLi4GJmZmbC2toZMxgc/iYiIWgutVkBafsnd0KQ/G1/yrdvQ1PBGXRtTY70JJCreC9XO3hSmcn6lpjt4JdSBs7MzAOgCFjU/1tbWut8jERERtRyCIOBmUalu8ohr94xAXc8pgrqGF+qaGMv0RqDa2ZnpJpOwMZM34lFQc8VwVQcSiQQuLi5wdHREWVn199lS02RsbMwRKyIiomauoKTsznNQOUW6kaiKQFVQUv0LdY1lErjZmurPxnf3z06WfKEuPRyGq4cgk8n4JZ2IiIiogajLNUjKKdabQKLiz1kF6mq3k0gAlZUJPB3ujj7dMxrlam0CIxmfg6KGwXBFRERERKLRaAWk6l6o+8/o0/WcIqTcuo0aHoOCvblcN+p072x87nZ8oS6Jg+GKiIiIiBqUIAjIKlDrTSBRMQKVlFOMUk31z0GZK4zuCVBm8HT4ZzY+S75Ql5oYhisiIiIiqhd5t8t074K6llWEaznFuj8X1fBCXbmRFO3sTKucjc/enC/UpeaD4YqIiIiohdFoBZy+dhOZBSVwtFCih4ctZPX0wtqSMg2u351EQjcb390RqZyi0mq3k0qANjameiNQFc9DqaxN6q0+IjExXBERERG1IBHn0rAsPB5pef+8+NbFSokloZ0x2NelVvso02hx49bteyaQKNRNaZ6aV/MLdZ0sFWhn98/texUjUG62JlAY8TkoatkYroiIiIhaiIhzaZgaFo3754BIzyvB1LBorBvfVRewtFoBGQUluhGo6/eMQiXdLEZ5DTNJWCqN4OlgfuddUPc8D9XO3gzmCn69pNaLVz8RERFRC6DRClgWHl8pWAHQtb2+Kw4/xKbges6dUanbZdU/B6U0llYxAnVnNj4bU2M+B0VUBYYrIiIiohbg9LUcvVsBq1Ko1uDAuQzdz0bSOy/U1ZuN7+4IlLOlElI+B0VkEIYrIiIiomaouLQcZ2/kISY5FzFJt3Dyak6tthsRoMJTASp42JujjY0JjPlCXaJ6w3BFRERE1MQJgoBr2UWIScpFTPItxCTl4mJ6ATQ1vWG3GqO7t0VPL7sGqJKIGK6IiIiImpi822WIS87VhanY5FzkFpdV6udkqUDXtjYIbGsNvzbWmLMjBhn56iqfu5IAcLa6My07ETUMhisiIiIiEWm0Av7OKLgTpJJuISY5F1cyCyv1UxhJ0cXVCoFtrRF4N1C5WJno9Vn6lA+mhkVDAugFrIonp5aEdub7pIgaEMMVERERUSPKKlAj9u5zUjFJuYi7kYvi0sqz9rnbmSLQ7Z8g5e1sCblRzc9HDfZ1wbrxXSu958rZwPdcEVHdMFwRERERNRB1uQbxqfl3w9SdW/ySb96u1M9cYQR/NysEut0JUgFu1rAzV9TpMwf7umBgZ2ecvnYTmQUlcLS4cysgR6yIGh7DFREREVE9EAQBKbm3797edydInU/JR6lGq9dPIgE6OJrrglRgWxu0dzSv1/Ajk0o4aQWRCEQPV5999hk+/PBDpKenw9/fH2vWrEGPHj2q7FtWVoYVK1bg66+/RkpKCjp27Ij3338fgwcP1vXRaDRYunQpwsLCkJ6eDpVKhZdeegmLFi3iy+6IiIio3hSXluPMjTy9Z6WyCtSV+tmaye/e3meNADcb+LlZwVJpLELFRNTQRA1XO3fuxNy5c7F+/XoEBwdj1apVCAkJwaVLl+Do6Fip/6JFixAWFoaNGzfC29sbBw8exMiRI/H7778jMDAQAPD+++9j3bp1+Prrr+Hj44O//voLkyZNgpWVFWbNmtXYh0hEREQtgFYr4FpO0T9BKikXlzIqT4VuJJWgs8pS71mptram/AdeolZCIgiC4S9IqCfBwcHo3r071q5dCwDQarVwc3PDzJkzMX/+/Er9VSoVFi5ciOnTp+vaRo0aBRMTE4SFhQEAhg8fDicnJ2zatKnaPg+Sn58PKysr5OXlwdLS8mEOkYiIiJqhvOIyxN74J0jFJuci73blqdBdrJR3bu27e4ufr6sVlMYyESomooZiSDYQbeSqtLQUUVFRWLBgga5NKpViwIABOHnyZJXbqNVqKJVKvTYTExMcP35c9/Njjz2GL774An///TceeeQRxMXF4fjx41i5cmW1tajVaqjV/wzj5+fn1/WwiIiIqJkp12jxd0ah7uW8MUm3cDWrqFI/hZEUfm2s7oxIuVkjoIqp0ImodRMtXGVnZ0Oj0cDJyUmv3cnJCRcvXqxym5CQEKxcuRJ9+vSBl5cXIiMjsWfPHmg0/0xfOn/+fOTn58Pb2xsymQwajQbvvPMOxo0bV20tK1aswLJly+rnwIiIiKhJyywoQWxSLmLuTod+5kZelVOht7Mz1d3aF+hmA28XCxjLap4KnYhaN9EntDDE6tWr8eqrr8Lb2xsSiQReXl6YNGkSNm/erOvz3XffYdu2bdi+fTt8fHwQGxuLOXPmQKVSYeLEiVXud8GCBZg7d67u5/z8fLi5uTX48RAREVHDqpgKPeaeMHXjVtVToQfcnXQisK01/NvUfSp0Imq9RAtX9vb2kMlkyMjI0GvPyMiAs7Nzlds4ODhg3759KCkpQU5ODlQqFebPnw9PT09dnzfeeAPz58/HmDFjAABdunRBYmIiVqxYUW24UigUUCj4FygREVFzZshU6I84WuiCVGBbG3g51O9U6ETUOokWruRyOYKCghAZGYkRI0YAuDOhRWRkJGbMmFHjtkqlEq6urigrK8Pu3bvx/PPP69YVFxdDKtUfspfJZNBqtffvhoiIiJqxukyFHtjWBn5trGDBqdCJqAGIelvg3LlzMXHiRHTr1g09evTAqlWrUFRUhEmTJgEAJkyYAFdXV6xYsQIAcOrUKaSkpCAgIAApKSlYunQptFot5s2bp9tnaGgo3nnnHbRt2xY+Pj6IiYnBypUr8fLLL4tyjERERPTwqpoK/WJ6Pu6bCZ1ToRORqEQNV6NHj0ZWVhYWL16M9PR0BAQEICIiQjfJRVJSkt4oVElJCRYtWoSEhASYm5tj6NCh2Lp1K6ytrXV91qxZg//85z+YNm0aMjMzoVKp8Nprr2Hx4sWNfXhERERUR3nFZbrZ+2KTORU6ETUPor7nqqnie66IiIgaT7lGi0sZBXrPSiVwKnQiaiKaxXuuiIiIqHXKLCj5J0jdnQr9dhmnQiei5o/hioiIiBqMulyD8xVTod99Violt/JU6BYKIwS0tdaNSAW42cDWTC5CxUREdcdwRURERPVCEATcuHVb9z6pmKRcxKdWPRV6RycLvWelvBzMIeVU6ETUzDFcERERUZ0Uqe9OhX534omYpFxkF1Y9FXrXu9OgB7pZowunQieiForhioiIiB5IqxWQkF2ke59UTFIuLlUzFbqPylLvWSk3WxNOhU5ErQLDFREREVWSW1yK2OSK2ftyEZt0C/kl5ZX6qayU/wSpttbwUXEqdCJqvRiuiIiIWrnaToWuNJbCz9VaF6QC3GzgbKUUoWIioqaJ4YqIiKiVycwv0d3aV9NU6B72Zgh0qwhTNujozKnQiYhqwnBFRETUgtVlKvTAtjbwd7PmVOhERAZiuCIiImohKqZCj74bomKScxGfmocyjf6sE5wKnYioYTBcERERNVNF6nLE3cjVPSsVm3wL2YWllfrZmcl1t/YFulnDz80a5gp+BSAiqm/8m5WIiKgZuDMVeiGik/55VurvjIJKU6EbyyTo7MKp0ImIxMBwRURE1ATlFpfqTToRm5yLggdOhW4DH5Ulp0InIhIJwxUREZHIyjVaXEwvuBumbiE2KRcJ2dVMhd7GWu9ZKSdLToVORNRUMFwRERE1ssz8kju39yXfmXjibDVToXvam92Zwe/us1KcCp2IqGljuCIiImpAJWUVU6HfQkxyLmKrmwpdaYSAu9OgB7a1RkAba9hwKnQiomaF4YqIiKgKGq2A09duIrOgBI4WSvTwsIXsAVOV13YqdKkEeMTJQhekura1hqc9p0InImruGK6IiIjuE3EuDcvC45GWV6Jrc7FSYkloZwz2ddG1FarLcaYWU6Hbm8sR4FYx6YQ1/NpwKnQiopaIf7MTERHdI+JcGqaGReO+Gc6RnleCKWHReLGnO8o1Qs1ToausEOhmfXdUygZtbDgVOhFRa8BwRUREdJdGK2BZeHylYAVA17b1ZKJeu6u1yZ1JJ9w4FToRUWvHcEVERHTXyavZercCVifU3wXDuqg4FToREelhuCIiolZNqxVw+vpNhMelYl9sSq22GdDJCYN9nRu4MiIiam4YroiIqNURBAGxybkIj0vDT2dTkZGvNmh7RwuOVhERUWUMV0RE1CoIgoD4tHzsP5OG8LhU3Lj1z7umLJRGGOzjjGFdXDB/z1lk5JdU+dyVBICz1Z1p2YmIiO7HcEVERC3alcxChMelIvxMKhKyinTtpnIZBnZ2QqifCr0fsYfC6M4kFEuf6oypYdGQAHoBq2KuvyWhnR/4visiImqdGK6IiKjFSb5ZjPAzqQiPS8OFtHxdu9xIiic7OiLUX4UnvR1hIq88q99gXxesG9+10nuunKt4zxUREdG9GK6IiKhFSM8rwf4zqQg/k4a45Fxdu5FUgj6POGC4nwsGdnaChdL4gfsa7OuCgZ2dcfraTWQWlMDR4s6tgByxIiKimjBcERFRs5VdqMaBc+kIj0vFn9dvQrh7H59UAvT0skOonwohPs6wMZMbvG+ZVIKeXnb1XDEREbVkDFdERNSs5BWX4eD5dISfScXvV3Og0f7zZFQ3dxuE+qswpIszZ/QjIqJGx3BFRERNXpG6HL9eyEB4XCqO/p2FMs0/gcqvjRVC/VQY5ucClbWJiFUSEVFrx3BFRERNUkmZBkcuZSI8Lg2RFzNQUqbVrevoZIFQfxcM91Ohnb2ZiFUSERH9g+GKiIiajNJyLY5fyUJ4XBoOxWegUF2uW9fOzhRP+asw3F+FR5wsRKySiIioagxXREQkKo1WwB8JOQiPS8WBc+nIu12mW+dqbYLhfi4I9VfBR2UJiYSz9RERUdPFcEVERI1OqxUQlXQL++NS8dPZdGQXqnXrHCwUGNbFBaH+Lgh0s4GU058TEVEzwXBFRESNQhAEnE3JQ3hcKvafSdN7Qa+1qTGG+N4JVMEednyfFBERNUsMV0RE1KAupRcgPC4V4WdSkZhTrGs3VxhhkI8TQv1V6NXeHsYyqYhVEhERPTyGKyIiqncJWYXYfyYN4XGpuJxZqGtXGksxoJMThvup8ERHByiNZSJWSUREVL8YroiIqF7cuFWMn86kIfxMKs6l5Ova5TIp+nZ0QKi/Cv29HWGm4P96iIioZeL/4YiIqM4y80vw09k7I1TRSbm6dplUgsfb2yPUzwWDfJxhZWIsXpFERESNhOGKiIgMcrOoFBHn0hEel4o/ruVAEO60SyRAsIctQv1VGOzjDDtzhbiFEhERNTKGKyIieqD8kjL8cj4D+8+k4vjlbJRrBd26wLbWCPVTYZifC5wslSJWSUREJC6GKyIiqlJxaTkiL2QiPC4VRy5loVSj1a3zUVki1F+FYV1c4GZrKmKVRERETQfDFRER6ZSUaXD07yzsP5OGX+MzcLtMo1vn5WCGp/xdMdzfBV4O5iJWSURE1DQxXBERtXJlGi1OXMlGeFwafjmfjgJ1uW5dW1tThPq7YLifCt7OFpBI+HJfIiKi6jSJNzZ+9tlnaNeuHZRKJYKDg3H69Olq+5aVlWH58uXw8vKCUqmEv78/IiIi9Pq0a9cOEomk0jJ9+vSGPhQiomZBoxVw8moO3tp7Fj3e+RUvbfkTu6NvoEBdDmdLJf7VywM/TH8cR994Am+EeKOTiyWDFRER0QOIPnK1c+dOzJ07F+vXr0dwcDBWrVqFkJAQXLp0CY6OjpX6L1q0CGFhYdi4cSO8vb1x8OBBjBw5Er///jsCAwMBAH/++Sc0mn9uZTl37hwGDhyI5557rtGOi4ioqREEAdFJudh/JhU/nUlDZoFat87eXI4hvi4I9Vehm7sNpFIGKSIiIkNJBEEQHtyt4QQHB6N79+5Yu3YtAECr1cLNzQ0zZ87E/PnzK/VXqVRYuHCh3ijUqFGjYGJigrCwsCo/Y86cOdi/fz8uX75c5b+8qtVqqNX/fMnIz8+Hm5sb8vLyYGlp+bCHSEQkGkEQcD41H+FnUrE/Lg0pubd16yyVRrpA9ainLYxkTeJmBiIioiYlPz8fVlZWtcoGoo5clZaWIioqCgsWLNC1SaVSDBgwACdPnqxyG7VaDaVSf6pfExMTHD9+vNrPCAsLw9y5c6u9pWXFihVYtmxZHY+CiKjpuZxRgPAzadgfl4qE7CJdu5lchoGdnRDqr0LvDg6QGzFQERER1RdRw1V2djY0Gg2cnJz02p2cnHDx4sUqtwkJCcHKlSvRp08feHl5ITIyEnv27NG7DfBe+/btQ25uLl566aVq61iwYAHmzp2r+7li5IqIqDlJzCnC/jNpCI9LxcX0Al27wkiK/p0cEeqnQj9vRyiNZSJWSURE1HKJ/syVoVavXo1XX30V3t7ekEgk8PLywqRJk7B58+Yq+2/atAlDhgyBSqWqdp8KhQIKhaKhSiYiajBpebfx091AFXcjT9duLJOgTwcHhPqrMKCzE8wVze6veyIiomZH1P/b2tvbQyaTISMjQ689IyMDzs7OVW7j4OCAffv2oaSkBDk5OVCpVJg/fz48PT0r9U1MTMSvv/6KPXv2NEj9RERiyCpQ48C5NOyPS8Pp6zd17VIJ8Hh7e4T6qRDi4wwrU2MRqyQiImp9RA1XcrkcQUFBiIyMxIgRIwDcmdAiMjISM2bMqHFbpVIJV1dXlJWVYffu3Xj++ecr9dmyZQscHR0xbNiwhiifiKjR5BaX4uD5dITHpeH3q9nQ3jMVUY92tgj1d8FgXxc4WHAUnoiISCyi3ycyd+5cTJw4Ed26dUOPHj2watUqFBUVYdKkSQCACRMmwNXVFStWrAAAnDp1CikpKQgICEBKSgqWLl0KrVaLefPm6e1Xq9Viy5YtmDhxIoyMRD9MIiKDFarLcSg+Hfvj0nDschbKNP8kKn83a4T6uWCYnwtcrExErJKIiIgqiJ46Ro8ejaysLCxevBjp6ekICAhARESEbpKLpKQkSKX/zGZVUlKCRYsWISEhAebm5hg6dCi2bt0Ka2trvf3++uuvSEpKwssvv9yYh0NE9FBKyjQ4fDET4XGpOHwxE+pyrW6dt7MFQv1VCPVToa2dqYhVEhERUVVEf89VU2TIXPZERA+rtFyL/13OQnhcKg7FZ6Co9J/ZTz3tzTDcX4VQPxd0cLIQsUoiIqLWqdm854qIqLUq12hxMiEH4XGpiDiXjvySct06V2uTOyNU/i7o7GJZ7Tv6iIiIqGlhuCIiaiRarYC/Em8hPC4VP59NQ05RqW6do4UCw/xcEOqvQqCbNQMVERFRM8RwRUTUgARBQNyNPITHpeKnM2lIzy/RrbMxNcaQLi4I9VOhh4ctZFIGKiIiouaM4YqIqJ4JgoCL6QUIj0tF+JlUJN+8rVtnoTBCiK8zQv1VeMzLDsYyaQ17IiIiouaE4YqIqJ5czSpEeFwq9p9Jw5XMQl27ibEMAzo7IdTPBX0ecYDSWCZilURERNRQGK6IiB5C8s1i7D+ThvC4VMSn5eva5UZS9OvogFB/FZ70doSpnH/dEhERtXT8vz0RkYEy8kvw05k0hJ9JRUxSrq7dSCpBrw72CPVTYaCPEyyVxuIVSURERI2O4YqIqBZyCtU4cC4d+8+k4tS1m6h4Q6BEAvT0tEOovwqDfZxhYyYXt1AiIiISDcMVEVE18m6X4Zfz6Qg/k4YTV7Kh0f7zzvUgdxuE+rlgaBcXOFoqRaySiIiImgqGKyKiexSXluPXC5kIj0vF0UtZKNVodeu6uFphuJ8Lhvm5oI2NqYhVEhERUVPEcEVErV5JmQZHLmUh/EwqIi9koKTsn0DVwdEcT/mrMNxfBQ97MxGrJCIioqaO4YqIWqUyjRbHr2QjPC4Vv5zPQKG6XLfO3c4UoX4qhPqr0NHZQsQqiYiIqDlhuCKiVkOjFXAqIQfhZ1Jx4Fw6covLdOtUVkoM91ch1E8FX1dLSCQSESslIiKi5ojhiohaNK1WQEzyLYTHpeGns2nIKlDr1tmbyzGsiwtC/VXo2tYGUikDFREREdUdwxURtTiCIOBcSj7Cz6TipzNpSMm9rVtnZWKMoV2cEeqnQrCnHWQMVERERFRPGK6IqMX4O6MA4XGpCI9LxfWcYl27ucIIgzo7IdRfhcfb20NuJBWxSiIiImqpGK6IqFm7nl2E/WdSER6XhksZBbp2pbEU/Ts5IdTPBU90dITSWCZilURERNQaMFwRUbOTknsbP90NVGdT8nTtxjIJ+j7iiFB/Fwzo5AQzBf+KIyIiosbDbx5E1CxkFpTgwNl0hMel4q/EW7p2mVSCx7zsEOqvQkhnZ1iZGotYJREREbVmDFdE1GTdKipFxPl07D+TipNXc6AV7rRLJECPdrYI9VdhiK8z7MwV4hZKREREBIYrImpiCkrKcCg+A+Fxqfjf5WyUVyQqAAFu1gj1V2FYFxc4WylFrJKIiIioMoYrImowGq2A09duIrOgBI4WSvTwsK1y6vPbpRocvpiJ8LhUHL6UidJyrW5dZxdLhPqrMNzPBW62po1ZPhEREZFBGK6IqEFEnEvDsvB4pOWV6NpcrJRYEtoZg31doC7X4Njf2QiPS8WvFzJQXKrR9fN0MMNT/ioM91OhvaO5GOUTERERGYzhiojqXcS5NEwNi4ZwX3t6XgmmhEWjp6cdzqXmoaCkXLeujY0JQv1VCPVToZOLBSQSvtyXiIiImheGKyKqVxqtgGXh8ZWCFQBd28mEHACAk6UCw/1UCPVXwb+NFQMVERERNWsMV0RUr05fu6l3K2B1Fg/vhJce84C0imewiIiIiJojqdgFEFHLklnw4GAFAHbmCgYrIiIialEYroioXjla1G6K9Nr2IyIiImouGK6IqF51b2cDE2NZtesluDNrYA8P28YrioiIiKgRMFwRUb3aFXUDt8s0Va6ruAlwSWjnKt93RURERNScMVwRUb2JTc7Fkh/OAwBGBKjgYqV/65+zlRLrxnfFYF8XMcojIiIialCcLZCI6kVOoRrTwqJQqtEixMcJn4wOgFa4M3tgZkEJHC3u3ArIESsiIiJqqRiuiOihabQCZu2IQWpeCTztzfDhc/6QSCSQSYCeXnZil0dERETUKHhbIBE9tI9+uYQTV3JgKpdh/YtBsFQai10SERERUaNjuCKihxJxLh3rjlwFALw/yg+POFmIXBERERGROBiuiKjOrmYV4vVdcQCAV3p5INRfJXJFREREROJhuCKiOilSl2PK1igUqsvRw8MW84d4i10SERERkagYrojIYIIg4M3dZ3A5sxCOFgqsfSEQxjL+dUJEREStG78NEZHBNh2/hv1n0mAklWDd+K5wtFA+eCMiIiKiFo7hiogMciohBysOXAQA/Gd4ZwS524pcEREREVHTwHBFRLWWkV+C6dtjoNEKGBGgwoSe7mKXRERERNRkMFwRUa2UlmsxbVs0sgvV8Ha2wLvPdIFEIhG7LCIiIqImg+GKiGrl3Z8vICrxFiyURlg/PgimciOxSyIiIiJqUkQPV5999hnatWsHpVKJ4OBgnD59utq+ZWVlWL58Oby8vKBUKuHv74+IiIhK/VJSUjB+/HjY2dnBxMQEXbp0wV9//dWQh0HUou2NuYGvfr8OAFg1OgDt7M3ELYiIiIioCRI1XO3cuRNz587FkiVLEB0dDX9/f4SEhCAzM7PK/osWLcKGDRuwZs0axMfHY8qUKRg5ciRiYmJ0fW7duoXHH38cxsbGOHDgAOLj4/Hxxx/DxsamsQ6LqEW5kJaPBXvOAgBmPdke/Ts5iVwRERERUdMkEQRBEOvDg4OD0b17d6xduxYAoNVq4ebmhpkzZ2L+/PmV+qtUKixcuBDTp0/XtY0aNQomJiYICwsDAMyfPx8nTpzA//73vzrXlZ+fDysrK+Tl5cHS0rLO+yFq7vJul+GptceRmFOMPo84YMtL3SGT8jkrIiIiaj0MyQaijVyVlpYiKioKAwYM+KcYqRQDBgzAyZMnq9xGrVZDqdR/n46JiQmOHz+u+/nHH39Et27d8Nxzz8HR0RGBgYHYuHFjjbWo1Wrk5+frLUStnVYrYO7OWCTmFKONjQlWjw5gsCIiIiKqgWjhKjs7GxqNBk5O+rcYOTk5IT09vcptQkJCsHLlSly+fBlarRaHDh3Cnj17kJaWpuuTkJCAdevWoUOHDjh48CCmTp2KWbNm4euvv662lhUrVsDKykq3uLm51c9BEjVjn/12BZEXMyE3kmL9+CDYmMnFLomIiIioSTM4XLVr1w7Lly9HUlJSQ9RTo9WrV6NDhw7w9vaGXC7HjBkzMGnSJEil/xyGVqtF165d8e677yIwMBCTJ0/Gq6++ivXr11e73wULFiAvL0+3JCcnN8bhEDVZR//Owspf/wYA/HeEL3xdrUSuiIiIiKjpMzhczZkzB3v27IGnpycGDhyIHTt2QK1WG/zB9vb2kMlkyMjI0GvPyMiAs7Nzlds4ODhg3759KCoqQmJiIi5evAhzc3N4enrq+ri4uKBz585623Xq1KnGMKhQKGBpaam3ELVWyTeLMXtHDAQBGNujLZ7vxpFcIiIiotqoU7iKjY3F6dOn0alTJ8ycORMuLi6YMWMGoqOja70fuVyOoKAgREZG6tq0Wi0iIyPRs2fPGrdVKpVwdXVFeXk5du/ejaefflq37vHHH8elS5f0+v/9999wd3evdW1ErVVJmQZTt0Uht7gM/m2ssPSpzg/eiIiIiIgAPMQzV127dsWnn36K1NRULFmyBF9++SW6d++OgIAAbN68GbWZhHDu3LnYuHEjvv76a1y4cAFTp05FUVERJk2aBACYMGECFixYoOt/6tQp7NmzBwkJCfjf//6HwYMHQ6vVYt68ebo+//73v/HHH3/g3XffxZUrV7B9+3Z88cUXejMMElFlgiDgP/vO4VxKPmzN5Fg3PggKI5nYZRERERE1G0Z13bCsrAx79+7Fli1bcOjQITz66KN45ZVXcOPGDbz11lv49ddfsX379hr3MXr0aGRlZWHx4sVIT09HQEAAIiIidJNcJCUl6T1PVVJSgkWLFiEhIQHm5uYYOnQotm7dCmtra12f7t27Y+/evViwYAGWL18ODw8PrFq1CuPGjavroRK1Ct+eTsauqBuQSoA1YwOhsjYRuyQiIiKiZsXg91xFR0djy5Yt+PbbbyGVSjFhwgT861//gre3t67PuXPn0L17d9y+fbveC24MfM8VtTaxybl4fv1JlGq0eHOwN6Y+4SV2SURERERNgiHZwOCRq+7du2PgwIFYt24dRowYAWNj40p9PDw8MGbMGEN3TUQiyClUY1pYFEo1WoT4OGFKX88Hb0RERERElRgcrhISEh44OYSZmRm2bNlS56KIqHGUa7SY+W0MUvNK4Glvho+e84dEwhcFExEREdWFwRNaZGZm4tSpU5XaT506hb/++qteiiKixvHxob/x+9UcmMplWP9iECyUlUeiiYiIiKh2DA5X06dPr/IluykpKZyRj6gZiTiXjnVHrgIAPnjWD484WYhcEREREVHzZnC4io+PR9euXSu1BwYGIj4+vl6KIqKGdTWrEK/vigMA/KuXB4b7qUSuiIiIiKj5MzhcKRQKZGRkVGpPS0uDkVGdZ3YnokZSpC7HlK1RKFSXo4eHLd4c4v3gjYiIiIjogQwOV4MGDcKCBQuQl5ena8vNzcVbb72FgQMH1mtxRFS/BEHAvN1ncDmzEE6WCnz2QlcYy+r8LnEiIiIiuofBQ00fffQR+vTpA3d3dwQGBgIAYmNj4eTkhK1bt9Z7gURUfzYdv4afzqTBSCrB5+O6wsFCIXZJRERERC2GweHK1dUVZ86cwbZt2xAXFwcTExNMmjQJY8eOrfKdV0TUNPyRkIMVBy4CAP4zvDOC3G1FroiIiIioZanTQ1JmZmaYPHlyfddCRA0kPa8EM7ZHQ6MVMDLQFRN61vyuOiIiIiIyXJ1noIiPj0dSUhJKS0v12p966qmHLoqI6k9puRbTt0cju7AU3s4WeHdkF74omIiIiKgBGByuEhISMHLkSJw9exYSiQSCIACA7suaRqOp3wqJ6KG881M8ohJvwUJphPXjg2Ail4ldEhEREVGLZPA0YbNnz4aHhwcyMzNhamqK8+fP49ixY+jWrRuOHDnSACUSUV3tjbmBr08mAgBWjQ5AO3szkSsiIiIiarkMHrk6efIkDh8+DHt7e0ilUkilUvTq1QsrVqzArFmzEBMT0xB1EpGB4lPzsWDPWQDArCfbo38nJ5ErIiIiImrZDB650mg0sLCwAADY29sjNTUVAODu7o5Lly7Vb3VEVCd5xWWYEhaFkjIt+j7igNkDHhG7JCIiIqIWz+CRK19fX8TFxcHDwwPBwcH44IMPIJfL8cUXX8DT07MhaiQiA2i1AuZ+F4ukm8VoY2OC1WMCIJNyAgsiIiKihmZwuFq0aBGKiooAAMuXL8fw4cPRu3dv2NnZYefOnfVeIBEZZu1vVxB5MRMKIynWjw+Ctalc7JKIiIiIWgWDw1VISIjuz+3bt8fFixdx8+ZN2NjYcHpnIpEduZSJT379GwDw3xG+8HW1ErkiIiIiotbDoGeuysrKYGRkhHPnzum129raMlgRiSz5ZjFm74iFIAAvBLfFc93cxC6JiIiIqFUxKFwZGxujbdu2fJcVURNTUqbBlLAo5N0ug7+bNZaEdha7JCIiIqJWx+DZAhcuXIi33noLN2/ebIh6iMhAgiBg0b5zOJ+aD1szOdaN6wqFEV8UTERERNTYDH7mau3atbhy5QpUKhXc3d1hZqb/UtLo6Oh6K46IHmz76SR8H3UDUgmwdmwgVNYmYpdERERE1CoZHK5GjBjRAGUQUV3EJN3C0h/PAwDmDfbGY+3tRa6IiIiIqPUyOFwtWbKkIeogIgPlFKoxbVs0yjQCQnyc8FofvmeOiIiISEwGP3NFROIr12gx89sYpOWVwNPBDB89588ZO4mIiIhEZvDIlVQqrfFLHGcSJGp4H/3yN36/mgNTuQwbxgfBQmksdklERERErZ7B4Wrv3r16P5eVlSEmJgZff/01li1bVm+FEVHVIs6lYf3RqwCAD5/1RwcnC5ErIiIiIiKgDuHq6aefrtT27LPPwsfHBzt37sQrr7xSL4URUWVXMgvx+q4zAIBXe3tgmJ+LyBURERERUYV6e+bq0UcfRWRkZH3tjojuU6Qux5SwKBSqyxHsYYs3B3uLXRIRERER3aNewtXt27fx6aefwtXVtT52R0T3EQQB874/gyuZhXCyVGDtC11hJON8NERERERNicG3BdrY2OhNaCEIAgoKCmBqaoqwsLB6LY6I7th0/Bp+OpsGY5kEn48LgoOFQuySiIiIiOg+BoerTz75RC9cSaVSODg4IDg4GDY2NvVaHBEBfyTkYMWBiwCA/wzvjCB3/ndGRERE1BQZHK5eeumlBiiDiKqSnleCGdujodEKGBnoihcfdRe7JCIiIiKqhsEPbWzZsgW7du2q1L5r1y58/fXX9VIUEQGl5VpM2xaF7MJSeDtb4N2RXfiiYCIiIqImzOBwtWLFCtjb21dqd3R0xLvvvlsvRRER8N+f4hGdlAtLpRE2vBgEE7lM7JKIiIiIqAYGh6ukpCR4eHhUand3d0dSUlK9FEXU2u2JvoFvTiYCAFaNCYC7nZnIFRERERHRgxgcrhwdHXHmzJlK7XFxcbCzs6uXoohas/jUfLy19ywAYFb/DnjS20nkioiIiIioNgwOV2PHjsWsWbPw22+/QaPRQKPR4PDhw5g9ezbGjBnTEDUStRp5xWWYEhaFkjIt+j7igNn9O4hdEhERERHVksGzBb799tu4fv06+vfvDyOjO5trtVpMmDCBz1wRPQStVsC/v4tF0s1itLExweoxAZBJOYEFERERUXMhEQRBqMuGly9fRmxsLExMTNClSxe4u7ecKaLz8/NhZWWFvLw8WFpail0OtRKfRl7GykN/Q2Ekxe6pj8HX1UrskoiIiIhaPUOygcEjVxU6dOiADh14yxJRffjtUiY++fVvAMA7I7swWBERERE1QwY/czVq1Ci8//77ldo/+OADPPfcc/VSFFFrknyzGHN2xEIQgHHBbfFsUBuxSyIiIiKiOjA4XB07dgxDhw6t1D5kyBAcO3asXooiai1KyjR4bWsU8m6XIcDNGotDO4tdEhERERHVkcHhqrCwEHK5vFK7sbEx8vPz61TEZ599hnbt2kGpVCI4OBinT5+utm9ZWRmWL18OLy8vKJVK+Pv7IyIiQq/P0qVLIZFI9BZvb+861UbUUARBwMK95xCflg87MznWje8KhRFfFExERETUXBkcrrp06YKdO3dWat+xYwc6dzb8X9137tyJuXPnYsmSJYiOjoa/vz9CQkKQmZlZZf9FixZhw4YNWLNmDeLj4zFlyhSMHDkSMTExev18fHyQlpamW44fP25wbUQNafvpJOyOvgGpBFgzNhAuViZil0RERERED8HgCS3+85//4JlnnsHVq1fx5JNPAgAiIyOxfft2fP/99wYXsHLlSrz66quYNGkSAGD9+vX46aefsHnzZsyfP79S/61bt2LhwoW6WxOnTp2KX3/9FR9//DHCwsL+OTAjIzg7OxtcD1FjiEm6haU/ngcAzBvsjcfa24tcERERERE9LINHrkJDQ7Fv3z5cuXIF06ZNw//93/8hJSUFhw8fRvv27Q3aV2lpKaKiojBgwIB/CpJKMWDAAJw8ebLKbdRqNZRKpV6biYlJpZGpy5cvQ6VSwdPTE+PGjUNSUlK1dajVauTn5+stRA0lu1CNaduiUaYRMNjHGa/18RS7JCIiIiKqBwaHKwAYNmwYTpw4gaKiIiQkJOD555/H66+/Dn9/f4P2k52dDY1GAycnJ712JycnpKenV7lNSEgIVq5cicuXL0Or1eLQoUPYs2cP0tLSdH2Cg4Px1VdfISIiAuvWrcO1a9fQu3dvFBQUVLnPFStWwMrKSre4ubkZdBxEtVWu0WLm9hik5ZXAy8EMHz7nB4mELwomIiIiagnqFK6AO7MGTpw4ESqVCh9//DGefPJJ/PHHH/VZW5VWr16NDh06wNvbG3K5HDNmzMCkSZMglf5zKEOGDMFzzz0HPz8/hISE4Oeff0Zubi6+++67Kve5YMEC5OXl6Zbk5OQGPw5qnT785RJOJuTATC7DhheDYKE0FrskIiIiIqonBj1zlZ6ejq+++gqbNm1Cfn4+nn/+eajVauzbt69Ok1nY29tDJpMhIyNDrz0jI6Pa56UcHBywb98+lJSUICcnByqVCvPnz4enZ/W3VllbW+ORRx7BlStXqlyvUCigUCgMrp/IEAfOpmHD0QQAwAfP+qO9o4XIFRERERFRfar1yFVoaCg6duyIM2fOYNWqVUhNTcWaNWse6sPlcjmCgoIQGRmpa9NqtYiMjETPnj1r3FapVMLV1RXl5eXYvXs3nn766Wr7FhYW4urVq3BxcXmoeonq6kpmIV7fFQcAeLW3B4b58VokIiIiamlqPXJ14MABzJo1C1OnTkWHDh3qrYC5c+di4sSJ6NatG3r06IFVq1ahqKhIN3vghAkT4OrqihUrVgAATp06hZSUFAQEBCAlJQVLly6FVqvFvHnzdPt8/fXXERoaCnd3d6SmpmLJkiWQyWQYO3ZsvdVNVFuF6nJMCYtCUakGj3ra4s3BfOcaERERUUtU63B1/PhxbNq0CUFBQejUqRNefPFFjBkz5qELGD16NLKysrB48WKkp6cjICAAERERukkukpKS9J6nKikpwaJFi5CQkABzc3MMHToUW7duhbW1ta7PjRs3MHbsWOTk5MDBwQG9evXCH3/8AQcHh4eul8gQgiDgze/P4EpmIZwsFVgztiuMZHV+1JGIiIiImjCJIAiCIRsUFRVh586d2Lx5M06fPg2NRoOVK1fi5ZdfhoVFy3iGJD8/H1ZWVsjLy4OlpaXY5VAztvFYAt75+QKMZRLsmNwTQe42YpdERERERAYwJBsYHK7udenSJWzatAlbt25Fbm4uBg4ciB9//LGuu2syGK6oPpy8moPxm05BoxXw9tM+eLFnO7FLIiIiIiIDGZINHur+pI4dO+KDDz7AjRs38O233z7MrohalLS825j5bTQ0WgHPBLpi/KPuYpdERERERA3soUauWiqOXNHDKC3XYvQXJxGTlItOLpbYM/UxmMhlYpdFRERERHXQaCNXRFTZf3+KR0xSLiyVRlg/viuDFREREVErwXBFVI/2RN/ANycTAQCrxgTA3c5M5IqIiIiIqLEwXBHVk/OpeViw5ywAYHb/DnjS20nkioiIiIioMTFcEdWDvOIyTA2Lhrpciyc6OmB2//p70TYRERERNQ8MV0QPSasVMGdnDJJuFsPN1gSrRgdAKpWIXRYRERERNTKGK6KHtObwFfx2KQsKIynWjQuCtalc7JKIiIiISAQMV0QP4bdLmVgV+TcA4J2RXeDraiVyRUREREQkFoYrojpKyinG7G9jIAjA+Efb4tmgNmKXREREREQiYrgiqoOSMg2mhEUhv6QcAW7W+M/wzmKXREREREQiY7giMpAgCFi49xzi0/JhZybHuvFdoTDii4KJiIiIWjuGKyIDbTuVhN3RNyCVAGteCISLlYnYJRERERFRE8BwRWSAmKRbWBZ+HgDw5mBvPOZlL3JFRERERNRUMFwR1VJ2oRpTw6JRphEwxNcZk/t4il0SERERETUhDFdEtVCu0WLm9hik55fAy8EMHz7nD4mELwomIiIion8wXBHVwocHL+FkQg7M5DJseDEI5gojsUsiIiIioiaG4YroAQ6cTcOGYwkAgA+f80d7RwuRKyIiIiKipojhiqgGVzIL8fquOADA5D6eGNrFReSKiIiIiKipYrgiqkahuhyvbf0LRaUaPOppi3khHcUuiYiIiIiaMIYroioIgoB538fhalYRnC2VWDO2K4xk/M+FiIiIiKrHb4tEVfjyf9fw89l0GMsk+Hx8VzhYKMQuiYiIiIiaOIYrovucvJqD9yIuAgAWh/qga1sbkSsiIiIiouaA4YroHml5tzFjezQ0WgHPdHXF+OC2YpdERERERM0EwxXRXepyDaZti0ZOUSk6uVjinRFd+KJgIiIiIqo1hiuiu/67/wJiknJhqTTChvFBMJHLxC6JiIiIiJoRhisiALujbmDrH4kAgNVjAtHWzlTkioiIiIiouWG4olbvfGoe3tp7FgAwu38H9PN2FLkiIiIiImqOGK6oVcsrLsOUsCioy7Xo19EBs/t3ELskIiIiImqmGK6o1dJqBczZGYPkm7fhZmuCT0YHQCrlBBZEREREVDcMV9RqfXr4Mn67lAWFkRTrxwfB2lQudklERERE1IwxXFGr9NvFTKyOvAwAeHdkF/iorESuiIiIiIiaO4YranWScooxe0cMBAEY/2hbjApqI3ZJRERERNQCMFxRq3K7VIPXwqKQX1KOwLbWWDzcR+ySiIiIiKiFYLiiVkMQBCzcdxYX0vJhby7H5+O6Qm7E/wSIiIiIqH7wmyW1GmGnkrAnOgUyqQRrxnaFi5WJ2CURERERUQvCcEWtQnTSLSwPPw8AeHNwR/T0shO5IiIiIiJqaRiuqMXLKlBjWlg0yjQChnZxxqu9PcUuiYiIiIhaIIYratHKNVrM/DYa6fkl8HIwwwfP+kMi4YuCiYiIiKj+MVxRi/bhwUv4I+EmzOQybHgxCOYKI7FLIiIiIqIWiuGKWqyfz6Zhw7EEAMCHz/mjvaOFyBURERERUUvGcEUt0pXMAryxKw4A8FofTwzt4iJyRURERETU0jFcUYtTqC7Ha1ujUFSqQU9PO7wR0lHskoiIiIioFWgS4eqzzz5Du3btoFQqERwcjNOnT1fbt6ysDMuXL4eXlxeUSiX8/f0RERFRbf/33nsPEokEc+bMaYDKqakRBAFv7IrD1awiOFsqseaFQBjJmsRlTkREREQtnOjfOnfu3Im5c+diyZIliI6Ohr+/P0JCQpCZmVll/0WLFmHDhg1Ys2YN4uPjMWXKFIwcORIxMTGV+v7555/YsGED/Pz8GvowqInY+L8EHDiXDmOZBJ+P7wp7c4XYJRERERFRKyF6uFq5ciVeffVVTJo0CZ07d8b69ethamqKzZs3V9l/69ateOuttzB06FB4enpi6tSpGDp0KD7++GO9foWFhRg3bhw2btwIGxubxjgUEtnvV7Px3oGLAIDFoT7o2pa/dyIiIiJqPKKGq9LSUkRFRWHAgAG6NqlUigEDBuDkyZNVbqNWq6FUKvXaTExMcPz4cb226dOnY9iwYXr7ro5arUZ+fr7eQs1LWt5tzNweA60AjOraBuOD24pdEhERERG1MqKGq+zsbGg0Gjg5Oem1Ozk5IT09vcptQkJCsHLlSly+fBlarRaHDh3Cnj17kJaWpuuzY8cOREdHY8WKFbWqY8WKFbCystItbm5udT8oanTqcg2mhkUjp6gUnV0s8c5IX74omIiIiIganei3BRpq9erV6NChA7y9vSGXyzFjxgxMmjQJUumdQ0lOTsbs2bOxbdu2SiNc1VmwYAHy8vJ0S3JyckMeAtWzt/fHIzY5F1Ymxlg/PghKY5nYJRERERFRKyRquLK3t4dMJkNGRoZee0ZGBpydnavcxsHBAfv27UNRURESExNx8eJFmJubw9PTEwAQFRWFzMxMdO3aFUZGRjAyMsLRo0fx6aefwsjICBqNptI+FQoFLC0t9RZqHr6PuoGwP5IgkQCrxgSgrZ2p2CURERERUSslariSy+UICgpCZGSkrk2r1SIyMhI9e/ascVulUglXV1eUl5dj9+7dePrppwEA/fv3x9mzZxEbG6tbunXrhnHjxiE2NhYyGUc1WorzqXlYuPcsAGB2/w7o19FR5IqIiIiIqDUzEruAuXPnYuLEiejWrRt69OiBVatWoaioCJMmTQIATJgwAa6urrrnp06dOoWUlBQEBAQgJSUFS5cuhVarxbx58wAAFhYW8PX11fsMMzMz2NnZVWqn5iu3uBRTwqKgLteiX0cHzHqyg9glEREREVErJ3q4Gj16NLKysrB48WKkp6cjICAAERERukkukpKSdM9TAUBJSQkWLVqEhIQEmJubY+jQodi6dSusra1FOgJqbFqtgDk7Y5F88zba2ppi1ehASKWcwIKIiIiIxCURBEEQu4imJj8/H1ZWVsjLy+PzV03QJ4f+xurIy1AYSbFn2mPwUVmJXRIRERERtVCGZINmN1sgtW6HL2ZgdeRlAMC7I7swWBERERFRk8FwRc1GUk4x5uyIBQC8+Kg7RgW1EbcgIiIiIqJ7MFxRs3C7VIPXwqKQX1KOwLbW+M/wzmKXRERERESkh+GKmjxBELBw71lcSMuHvbkcn4/rCrkRL10iIiIialr4DZWavLBTSdgTkwKZVII1Y7vCxcpE7JKIiIiIiCphuKImLSrxFpaHnwcAzB/sjZ5ediJXRERERERUNYYrarKyCtSYti0KZRoBQ7s441+9PcQuiYiIiIioWgxX1CSVa7SY+W00MvLVaO9ojg+e9YdEwhcFExEREVHTxXBFTdIHBy/hj4SbMJPLsH58EMwVRmKXRERERERUI4YranJ+PpuGL44lAAA+es4f7R3NRa6IiIiIiOjBGK6oSbmSWYA3dsUBAF7r64khXVxEroiIiIiIqHYYrqjJKCgpw+StUSgq1aCnpx3eGNRR7JKIiIiIiGqN4YqaBEEQMO/7M0jIKoKLlRJrXgiEkYyXJxERERE1H/z2Sk3CF8cScOBcOoxlEnw+rivszRVil0REREREZBCGKxLd71ey8X7ERQDAklAfBLa1EbkiIiIiIiLDMVyRqFJzb2PmtzHQCsCorm0wLrit2CUREREREdUJwxWJRl2uwbRt0cgpKkVnF0u8M9KXLwomIiIiomaL4YpE8/b+eMQm58LKxBgbXgyC0lgmdklERERERHXGcEWi+D7qBsL+SIJEAqwaEwA3W1OxSyIiIiIieigMV9TozqXkYeHeswCAOf0fQb+OjiJXRERERET08BiuqFHlFpdi6rYoqMu1eNLbETOfbC92SURERERE9YLhihqNVitg9o5YJN+8jba2pvjk+QBIpZzAgoiIiIhaBoYrajSrIy/j6N9ZUBhJsX58EKxMjcUuiYiIiIio3jBcUaM4fDEDqyMvAwBWPNMFnVWWIldERERERFS/GK6owSXmFGHOjlgAwISe7nimaxtxCyIiIiIiagAMV9SgbpdqMCUsGvkl5eja1hqLhnUWuyQiIiIiogbBcEUNRhAELNx7FhfS8mFvLsfn44IgN+IlR0REREQtE7/pUoMJ+yMRe2JSIJNKsGZsVzhbKcUuiYiIiIiowTBcUYOISryF5fvjAQDzB3ujp5edyBURERERETUshiuqd1kFakzbFoUyjYBhXVzwr94eYpdERERERNTgGK6oXpVrtJixPRoZ+Wq0dzTH+8/6QSLhi4KJiIiIqOVjuKJ69X7ERZy6dhPmCiOsHx8Ec4WR2CURERERETUKhiuqNz+dScPG/10DAHz0nB/aO5qLXBERERERUeNhuKJ6cSWzAG98HwcAeK2vJwb7uohcERERERFR42K4oodWUFKGyVujUFyqwWNednhjUEexSyIiIiIianQMV/RQBEHAG7vOICGrCC5WSnw6NhBGMl5WRERERNT68FswPZQNxxIQcT4dcpkUn4/rCntzhdglERERERGJguGK6uz3K9n4IOIiAGDJU50R2NZG5IqIiIiIiMTDcEV1kpp7GzO/jYFWAJ4NaoMXerQVuyQiIiIiIlExXJHB1OUaTN0WjZyiUvioLPHfEb58UTARERERtXoMV2Sw5eHxiEvOhZWJMdaPD4LSWCZ2SUREREREomO4IoPs+isZ204lQSIBVo8JgJutqdglERERERE1CQxXVGvnUvKwaN85AMC/BzyCJzo6ilwREREREVHT0STC1WeffYZ27dpBqVQiODgYp0+frrZvWVkZli9fDi8vLyiVSvj7+yMiIkKvz7p16+Dn5wdLS0tYWlqiZ8+eOHDgQEMfRouWW1yKKWFRUJdr8aS3I2b0ay92SURERERETYro4Wrnzp2YO3culixZgujoaPj7+yMkJASZmZlV9l+0aBE2bNiANWvWID4+HlOmTMHIkSMRExOj69OmTRu89957iIqKwl9//YUnn3wSTz/9NM6fP99Yh9WiaLQCZu+IxY1bt9HW1hSfPB8AqZQTWBARERER3UsiCIIgZgHBwcHo3r071q5dCwDQarVwc3PDzJkzMX/+/Er9VSoVFi5ciOnTp+vaRo0aBRMTE4SFhVX7Oba2tvjwww/xyiuvPLCm/Px8WFlZIS8vD5aWlnU4qpZl5aG/8WnkZSiNpdgz9XF0VvGcEBEREVHrYEg2EHXkqrS0FFFRURgwYICuTSqVYsCAATh58mSV26jVaiiVSr02ExMTHD9+vMr+Go0GO3bsQFFREXr27FntPvPz8/UWuiPyQgY+jbwMAFjxTBcGKyIiIiKiaogarrKzs6HRaODk5KTX7uTkhPT09Cq3CQkJwcqVK3H58mVotVocOnQIe/bsQVpaml6/s2fPwtzcHAqFAlOmTMHevXvRuXPnKve5YsUKWFlZ6RY3N7f6OcBmLjGnCP/eGQsAmNjTHSMD24hbEBERERFREyb6M1eGWr16NTp06ABvb2/I5XLMmDEDkyZNglSqfygdO3ZEbGwsTp06halTp2LixImIj4+vcp8LFixAXl6ebklOTm6MQ2nSbpdq8NrWKOSXlKNrW2ssHFZ1MCUiIiIiojtEDVf29vaQyWTIyMjQa8/IyICzs3OV2zg4OGDfvn0oKipCYmIiLl68CHNzc3h6eur1k8vlaN++PYKCgrBixQr4+/tj9erVVe5ToVDoZhasWFozQRDw1t6zuJheAHtzOT4fFwS5UbPL4UREREREjUrUb8xyuRxBQUGIjIzUtWm1WkRGRlb7fFQFpVIJV1dXlJeXY/fu3Xj66adr7K/VaqFWq+ul7pZu6x+J2BuTAplUgrUvdIWzlfLBGxERERERtXJGYhcwd+5cTJw4Ed26dUOPHj2watUqFBUVYdKkSQCACRMmwNXVFStWrAAAnDp1CikpKQgICEBKSgqWLl0KrVaLefPm6fa5YMECDBkyBG3btkVBQQG2b9+OI0eO4ODBg6IcY3MSlXgTy8Pv3D65YIg3HvW0E7kiIiIiIqLmQfRwNXr0aGRlZWHx4sVIT09HQEAAIiIidJNcJCUl6T1PVVJSgkWLFiEhIQHm5uYYOnQotm7dCmtra12fzMxMTJgwAWlpabCysoKfnx8OHjyIgQMHNvbhNSuZBSWYti0a5VoBw/xc8EovD7FLIiIiIiJqNkR/z1VT1Brfc1Wm0WLcl6dw+tpNtHc0xw/TH4eZQvTsTUREREQkqmbznitqOj6IuIjT127CXGGEDS8GMVgRERERERmI4Yqw/0wqNv7vGgDgo+f84eVgLnJFRERERETND8NVK3c5owDzvj8DAJjS1wuDfaueAp+IiIiIiGrGcNWKFZSU4bWtUSgu1eAxLzu8PugRsUsiIiIiImq2GK5aKUEQ8PquOCRkF8HFSok1YwNhJOPlQERERERUV/w23UptOJaAg+czIJdJsW58EOzMFWKXRERERETUrDFctUInrmTjg4iLAIClT/kgwM1a3IKIiIiIiFoAhqtWJjX3NmZ+GwOtADwX1AZje7iJXRIRERERUYvAcNWKqMs1mLotGjeLSuHraom3R/hCIpGIXRYRERERUYvAcNWKLAuPR1xyLqxNjbFuXBCUxjKxSyIiIiIiajEYrlqJ7/5KxvZTSZBIgFWjA+Bmayp2SURERERELQrDVStwLiUPi/adAwD8e8AjeKKjo8gVERERERG1PAxXLdytolJMCYtCabkW/b0dMaNfe7FLIiIiIiJqkRiuWjCNVsDsnbG4ces23O1MsXJ0AKRSTmBBRERERNQQGK5asNW//o1jf2dBaSzF+vFBsDIxFrskIiIiIqIWi+GqhYq8kIFPD18BAKx4pgs6uViKXBERERERUcvGcNUCXc8uwpydsQCAiT3dMTKwjbgFERERERG1AgxXLcztUg2mhEWhoKQcQe42WDiss9glERERERG1CgxXLYggCHhr71lcTC+AvbkCn4/rCrkRf8VERERERI2B37xbkG9OJmJvTApkUgnWvhAIJ0ul2CUREREREbUaDFctRFTiTby9Px4AsGCINx71tBO5IiIiIiKi1oXhqgXILCjBtG3RKNcKGO7ngld6eYhdEhERERFRq8Nw1cyVabSYsT0GGflqdHA0x/uj/CCR8EXBRERERESNjeGqmXv/wEWcvnYT5gojrH8xCGYKI7FLIiIiIiJqlRiumrH9Z1Lx5fFrAICPnvOHl4O5yBUREREREbVeDFfN1N8ZBZj3/RkAwNQnvDDY11nkioiIiIiIWjeGq2aooKQMU7ZGobhUg8fb2+H/Bj4idklERERERK0ew1UzIwgCXt8Vh4TsIqislPh0TCCMZPw1EhERERGJjd/Km5n1RxNw8HwG5DIpPh8fBDtzhdglERERERERAE4t14RptAJOX7uJzIISOFooUa7R4sODFwEAS5/yQYCbtbgFEhERERGRDsNVExVxLg3LwuORlleia5NIAEEAnu/WBmN7uIlYHRERERER3Y/hqgmKOJeGqWHREO5rF+429GpvzxcFExERERE1MXzmqonRaAUsC4+vFKzuteLARWi0NfUgIiIiIqLGxnDVxJy+dlPvVsCqpOWV4PS1m41UERERERER1QbDVROTWVBzsDK0HxERERERNQ6GqybG0UJZr/2IiIiIiKhxMFw1MT08bOFipUR101VIALhYKdHDw7YxyyIiIiIiogdguGpiZFIJloR2BoBKAavi5yWhnSGTcrZAIiIiIqKmhOGqCRrs64J147vC2Ur/1j9nKyXWje+Kwb4uIlVGRERERETV4XuumqjBvi4Y2NkZp6/dRGZBCRwt7twKyBErIiIiIqKmieGqCZNJJejpZSd2GUREREREVAu8LZCIiIiIiKgeMFwRERERERHVgyYRrj777DO0a9cOSqUSwcHBOH36dLV9y8rKsHz5cnh5eUGpVMLf3x8RERF6fVasWIHu3bvDwsICjo6OGDFiBC5dutTQh0FERERERK2Y6OFq586dmDt3LpYsWYLo6Gj4+/sjJCQEmZmZVfZftGgRNmzYgDVr1iA+Ph5TpkzByJEjERMTo+tz9OhRTJ8+HX/88QcOHTqEsrIyDBo0CEVFRY11WERERERE1MpIBEEQxCwgODgY3bt3x9q1awEAWq0Wbm5umDlzJubPn1+pv0qlwsKFCzF9+nRd26hRo2BiYoKwsLAqPyMrKwuOjo44evQo+vTp88Ca8vPzYWVlhby8PFhaWtbxyIiIiIiIqLkzJBuIOnJVWlqKqKgoDBgwQNcmlUoxYMAAnDx5sspt1Go1lEr99z+ZmJjg+PHj1X5OXl4eAMDW1rbafebn5+stREREREREhhA1XGVnZ0Oj0cDJyUmv3cnJCenp6VVuExISgpUrV+Ly5cvQarU4dOgQ9uzZg7S0tCr7a7VazJkzB48//jh8fX2r7LNixQpYWVnpFjc3t4c7MCIiIiIianVEf+bKUKtXr0aHDh3g7e0NuVyOGTNmYNKkSZBKqz6U6dOn49y5c9ixY0e1+1ywYAHy8vJ0S3JyckOVT0RERERELZSo4cre3h4ymQwZGRl67RkZGXB2dq5yGwcHB+zbtw9FRUVITEzExYsXYW5uDk9Pz0p9Z8yYgf379+O3335DmzZtqq1DoVDA0tJSbyEiIiIiIjKEqOFKLpcjKCgIkZGRujatVovIyEj07Nmzxm2VSiVcXV1RXl6O3bt34+mnn9atEwQBM2bMwN69e3H48GF4eHg02DEQEREREREBgJHYBcydOxcTJ05Et27d0KNHD6xatQpFRUWYNGkSAGDChAlwdXXFihUrAACnTp1CSkoKAgICkJKSgqVLl0Kr1WLevHm6fU6fPh3bt2/HDz/8AAsLC93zW1ZWVjAxMXlgTRUTKHJiCyIiIiKi1q0iE9RqknWhCVizZo3Qtm1bQS6XCz169BD++OMP3bq+ffsKEydO1P185MgRoVOnToJCoRDs7OyEF198UUhJSdHbH4Aqly1bttSqnuTk5Gr3wYULFy5cuHDhwoULl9a3JCcnPzBHiP6eq6ZIq9UiNTUVFhYWkEgkotaSn58PNzc3JCcn81mwBsDz2/B4jhsWz2/D4vltWDy/DYvnt2Hx/DaspnR+BUFAQUEBVCpVtZPoVRD9tsCmSCqV1jgBhhg40UbD4vlteDzHDYvnt2Hx/DYsnt+GxfPbsHh+G1ZTOb9WVla16tfspmInIiIiIiJqihiuiIiIiIiI6gHDVROnUCiwZMkSKBQKsUtpkXh+Gx7PccPi+W1YPL8Ni+e3YfH8Niye34bVXM8vJ7QgIiIiIiKqBxy5IiIiIiIiqgcMV0RERERERPWA4YqIiIiIiKgeMFwRERERERHVA4YrkR07dgyhoaFQqVSQSCTYt2/fA7c5cuQIunbtCoVCgfbt2+Orr75q8DqbK0PP75EjRyCRSCot6enpjVNwM7NixQp0794dFhYWcHR0xIgRI3Dp0qUHbrdr1y54e3tDqVSiS5cu+Pnnnxuh2uanLuf3q6++qnT9KpXKRqq4eVm3bh38/Px0L6js2bMnDhw4UOM2vHZrz9Dzy2v34bz33nuQSCSYM2dOjf14DddNbc4vr+HaW7p0aaVz5e3tXeM2zeXaZbgSWVFREfz9/fHZZ5/Vqv+1a9cwbNgw9OvXD7GxsZgzZw7+9a9/4eDBgw1cafNk6PmtcOnSJaSlpekWR0fHBqqweTt69CimT5+OP/74A4cOHUJZWRkGDRqEoqKiarf5/fffMXbsWLzyyiuIiYnBiBEjMGLECJw7d64RK28e6nJ+gTtvs7/3+k1MTGykipuXNm3a4L333kNUVBT++usvPPnkk3j66adx/vz5Kvvz2jWMoecX4LVbV3/++Sc2bNgAPz+/GvvxGq6b2p5fgNewIXx8fPTO1fHjx6vt26yuXYGaDADC3r17a+wzb948wcfHR69t9OjRQkhISANW1jLU5vz+9ttvAgDh1q1bjVJTS5OZmSkAEI4ePVptn+eff14YNmyYXltwcLDw2muvNXR5zV5tzu+WLVsEKyurxiuqhbGxsRG+/PLLKtfx2n14NZ1fXrt1U1BQIHTo0EE4dOiQ0LdvX2H27NnV9uU1bDhDzi+v4dpbsmSJ4O/vX+v+zena5chVM3Py5EkMGDBAry0kJAQnT54UqaKWKSAgAC4uLhg4cCBOnDghdjnNRl5eHgDA1ta22j68huuuNucXAAoLC+Hu7g43N7cHjhTQHRqNBjt27EBRURF69uxZZR9eu3VXm/ML8Nqti+nTp2PYsGGVrs2q8Bo2nCHnF+A1bIjLly9DpVLB09MT48aNQ1JSUrV9m9O1ayR2AWSY9PR0ODk56bU5OTkhPz8ft2/fhomJiUiVtQwuLi5Yv349unXrBrVajS+//BJPPPEETp06ha5du4pdXpOm1WoxZ84cPP744/D19a22X3XXMJ9rq1ltz2/Hjh2xefNm+Pn5IS8vDx999BEee+wxnD9/Hm3atGnEipuHs2fPomfPnigpKYG5uTn27t2Lzp07V9mX167hDDm/vHYNt2PHDkRHR+PPP/+sVX9ew4Yx9PzyGq694OBgfPXVV+jYsSPS0tKwbNky9O7dG+fOnYOFhUWl/s3p2mW4IrpHx44d0bFjR93Pjz32GK5evYpPPvkEW7duFbGypm/69Ok4d+5cjfdMU93V9vz27NlTb2TgscceQ6dOnbBhwwa8/fbbDV1ms9OxY0fExsYiLy8P33//PSZOnIijR49WGwDIMIacX167hklOTsbs2bNx6NAhTprQAOpyfnkN196QIUN0f/bz80NwcDDc3d3x3Xff4ZVXXhGxsofHcNXMODs7IyMjQ68tIyMDlpaWHLVqID169GBgeIAZM2Zg//79OHbs2AP/da66a9jZ2bkhS2zWDDm/9zM2NkZgYCCuXLnSQNU1b3K5HO3btwcABAUF4c8//8Tq1auxYcOGSn157RrOkPN7P167NYuKikJmZqbeXRUajQbHjh3D2rVroVarIZPJ9LbhNVx7dTm/9+M1XHvW1tZ45JFHqj1Xzena5TNXzUzPnj0RGRmp13bo0KEa72GnhxMbGwsXFxexy2iSBEHAjBkzsHfvXhw+fBgeHh4P3IbXcO3V5fzeT6PR4OzZs7yGa0mr1UKtVle5jtfuw6vp/N6P127N+vfvj7NnzyI2Nla3dOvWDePGjUNsbGyVX/x5DddeXc7v/XgN115hYSGuXr1a7blqVteu2DNqtHYFBQVCTEyMEBMTIwAQVq5cKcTExAiJiYmCIAjC/PnzhRdffFHXPyEhQTA1NRXeeOMN4cKFC8Jnn30myGQyISIiQqxDaNIMPb+ffPKJsG/fPuHy5cvC2bNnhdmzZwtSqVT49ddfxTqEJm3q1KmClZWVcOTIESEtLU23FBcX6/q8+OKLwvz583U/nzhxQjAyMhI++ugj4cKFC8KSJUsEY2Nj4ezZs2IcQpNWl/O7bNky4eDBg8LVq1eFqKgoYcyYMYJSqRTOnz8vxiE0afPnzxeOHj0qXLt2TThz5owwf/58QSKRCL/88osgCLx2H5ah55fX7sO7fzY7XsP160Hnl9dw7f3f//2fcOTIEeHatWvCiRMnhAEDBgj29vZCZmamIAjN+9pluBJZxdTf9y8TJ04UBEEQJk6cKPTt27fSNgEBAYJcLhc8PT2FLVu2NHrdzYWh5/f9998XvLy8BKVSKdja2gpPPPGEcPjwYXGKbwaqOrcA9K7Jvn376s53he+++0545JFHBLlcLvj4+Ag//fRT4xbeTNTl/M6ZM0do27atIJfLBScnJ2Ho0KFCdHR04xffDLz88suCu7u7IJfLBQcHB6F///66L/6CwGv3YRl6fnntPrz7v/zzGq5fDzq/vIZrb/To0YKLi4sgl8sFV1dXYfTo0cKVK1d065vztSsRBEFovHEyIiIiIiKilonPXBEREREREdUDhisiIiIiIqJ6wHBFRERERERUDxiuiIiIiIiI6gHDFRERERERUT1guCIiIiIiIqoHDFdERERERET1gOGKiIiIiIioHjBcERERPSSJRIJ9+/aJXQYREYmM4YqIiJq1l156CRKJpNIyePBgsUsjIqJWxkjsAoiIiB7W4MGDsWXLFr02hUIhUjVERNRaceSKiIiaPYVCAWdnZ73FxsYGwJ1b9tatW4chQ4bAxMQEnp6e+P777/W2P3v2LJ588kmYmJjAzs4OkydPRmFhoV6fzZs3w8fHBwqFAi4uLpgxY4be+uzsbIwcORKmpqbo0KEDfvzxR926W7duYdy4cXBwcICJiQk6dOhQKQwSEVHzx3BFREQt3n/+8x+MGjUKcXFxGDduHMaMGYMLFy4AAIqKihASEgIbGxv8+eef2LVrF3799Ve98LRu3TpMnz4dkydPxtmzZ/Hjjz+iffv2ep+xbNkyPP/88zhz5gyGDh2KcePG4ebNm7rPj4+Px4EDB3DhwgWsW7cO9vb2jXcCiIioUUgEQRDELoKIiKiuXnrpJYSFhUGpVOq1v/XWW3jrrbcgkUgwZcoUrFu3Trfu0UcfRdeuXfH5559j48aNePPNN5GcnAwzMzMAwM8//4zQ0FCkpqbCyckJrq6umDRpEv773/9WWYNEIsGiRYvw9ttvA7gT2MzNzXHgwAEMHjwYTz31FOzt7bF58+YGOgtERNQU8JkrIiJq9vr166cXngDA1tZW9+eePXvqrevZsydiY2MBABcuXIC/v78uWAHA448/Dq1Wi0uXLkEikSA1NRX9+/evsQY/Pz/dn83MzGBpaYnMzEwAwNSpUzFq1ChER0dj0KBBGDFiBB577LE6HSsRETVdDFdERNTsmZmZVbpNr76YmJjUqp+xsbHezxKJBFqtFgAwZMgQJCYm4ueff8ahQ4fQv39/TJ8+HR999FG910tEROLhM1dERNTi/fHHH5V+7tSpEwCgU6dOiIuLQ1FRkW79iRMnIJVK0bFjR1hYWKBdu3aIjIx8qBocHBwwceJEhIWFYdWqVfjiiy8ean9ERNT0cOSKiIiaPbVajfT0dL02IyMj3aQRu3btQrdu3dCrVy9s27YNp0+fxqZNmwAA48aNw5IlSzBx4kQsXboUWVlZmDlzJl588UU4OTkBAJYuXYopU6bA0dERQ4YMQUFBAU6cOIGZM2fWqr7FixcjKCgIPj4+UKvV2L9/vy7cERFRy8FwRUREzV5ERARcXFz02jp27IiLFy8CuDOT344dOzBt2jS4uLjg22+/RefOnQEApqamOHjwIGbPno3u3bvD1NQUo0aNwsqVK3X7mjhxIkpKSvDJJ5/g9ddfh729PZ599tla1yeXy7FgwQJcv34dJiYm6N27N3bs2FEPR05ERE0JZwskIqIWTSKRYO/evRgxYoTYpRARUQvHZ66IiIiIiIjqAcMVERERERFRPeAzV0RE1KLx7nciImosHLkiIiIiIiKqBwxXRERERERE9YDhioiIiIiIqB4wXBEREREREdUDhisiIiIiIqJ6wHBFRERERERUDxiuiIiIiIiI6gHDFRERERERUT34fzNI1v8KvDZDAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!pip install matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot loss over epochs\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, epochs + 1), custom_callback.epoch_losses, label=\"Loss\", marker='o')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Over Epochs\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot accuracy over epochs\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, epochs + 1), custom_callback.epoch_accuracies, label=\"Accuracy\", marker='o')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training Accuracy Over Epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click here for solution</summary> </br>\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Flatten \n",
    "from tensorflow.keras.callbacks import Callback \n",
    "\n",
    "# Step 1: Set Up the Environment\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() \n",
    "x_train = x_train / 255.0 \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32) \n",
    "\n",
    "# Step 2: Define the Model\n",
    "model = Sequential([ \n",
    "    tf.keras.Input(shape=(28, 28)),  # Updated Input layer syntax\n",
    "    Flatten(), \n",
    "    Dense(128, activation='relu'), \n",
    "    Dense(10) \n",
    "]) \n",
    "\n",
    "# Step 3: Define Loss Function, Optimizer, and Metric\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) \n",
    "optimizer = tf.keras.optimizers.Adam() \n",
    "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy() \n",
    "\n",
    "# Step 4: Implement the Custom Callback\n",
    "class CustomCallback(Callback): \n",
    "    def on_epoch_end(self, epoch, logs=None): \n",
    "        print(f'End of epoch {epoch + 1}, loss: {logs.get(\"loss\")}, accuracy: {logs.get(\"accuracy\")}') \n",
    "\n",
    "# Step 5: Implement the Custom Training Loop with Custom Callback\n",
    "custom_callback = CustomCallback() \n",
    "\n",
    "for epoch in range(5): \n",
    "    for x_batch, y_batch in train_dataset: \n",
    "        with tf.GradientTape() as tape: \n",
    "            logits = model(x_batch, training=True) \n",
    "            loss = loss_fn(y_batch, logits) \n",
    "        grads = tape.gradient(loss, model.trainable_weights) \n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights)) \n",
    "        accuracy_metric.update_state(y_batch, logits) \n",
    "    custom_callback.on_epoch_end(epoch, logs={'loss': loss.numpy(), 'accuracy': accuracy_metric.result().numpy()}) \n",
    "    accuracy_metric.reset_state()  # Updated method\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Lab - Hyperparameter Tuning \n",
    "\n",
    "#### Enhancement: Add functionality to save the results of each hyperparameter tuning iteration as JSON files in a specified directory. \n",
    "\n",
    "#### Additional Instructions:\n",
    "\n",
    "Modify the tuning loop to save each iteration's results as JSON files.\n",
    "\n",
    "Specify the directory where these JSON files will be stored for easier retrieval and analysis of tuning results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¹ Start of Epoch 1\n",
      "ğŸ”¹ End of Epoch 1: Loss = 0.0777, Accuracy = 0.9111\n",
      "\n",
      "ğŸ”¹ Start of Epoch 2\n",
      "ğŸ”¹ End of Epoch 2: Loss = 0.0507, Accuracy = 0.9541\n",
      "\n",
      "ğŸ”¹ Start of Epoch 3\n",
      "ğŸ”¹ End of Epoch 3: Loss = 0.0424, Accuracy = 0.9666\n",
      "\n",
      "ğŸ”¹ Start of Epoch 4\n",
      "ğŸ”¹ End of Epoch 4: Loss = 0.0340, Accuracy = 0.9744\n",
      "\n",
      "ğŸ”¹ Start of Epoch 5\n",
      "ğŸ”¹ End of Epoch 5: Loss = 0.0239, Accuracy = 0.9794\n",
      "âœ… Hyperparameter tuning results saved to hyperparameter_results/results_lr0.001_units64_epochs5.json\n",
      "\n",
      "ğŸ”¹ Start of Epoch 1\n",
      "ğŸ”¹ End of Epoch 1: Loss = 0.0615, Accuracy = 0.9099\n",
      "\n",
      "ğŸ”¹ Start of Epoch 2\n",
      "ğŸ”¹ End of Epoch 2: Loss = 0.0488, Accuracy = 0.9554\n",
      "\n",
      "ğŸ”¹ Start of Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 14:45:29.258506: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹ End of Epoch 3: Loss = 0.0524, Accuracy = 0.9685\n",
      "\n",
      "ğŸ”¹ Start of Epoch 4\n",
      "ğŸ”¹ End of Epoch 4: Loss = 0.0498, Accuracy = 0.9760\n",
      "\n",
      "ğŸ”¹ Start of Epoch 5\n",
      "ğŸ”¹ End of Epoch 5: Loss = 0.0353, Accuracy = 0.9809\n",
      "âœ… Hyperparameter tuning results saved to hyperparameter_results/results_lr0.0005_units128_epochs5.json\n",
      "\n",
      "ğŸ”¹ Start of Epoch 1\n",
      "ğŸ”¹ End of Epoch 1: Loss = 0.1353, Accuracy = 0.8679\n",
      "\n",
      "ğŸ”¹ Start of Epoch 2\n",
      "ğŸ”¹ End of Epoch 2: Loss = 0.0818, Accuracy = 0.9308\n",
      "\n",
      "ğŸ”¹ Start of Epoch 3\n",
      "ğŸ”¹ End of Epoch 3: Loss = 0.0585, Accuracy = 0.9444\n",
      "\n",
      "ğŸ”¹ Start of Epoch 4\n",
      "ğŸ”¹ End of Epoch 4: Loss = 0.0457, Accuracy = 0.9529\n",
      "\n",
      "ğŸ”¹ Start of Epoch 5\n",
      "ğŸ”¹ End of Epoch 5: Loss = 0.0380, Accuracy = 0.9599\n",
      "âœ… Hyperparameter tuning results saved to hyperparameter_results/results_lr0.0001_units256_epochs5.json\n",
      "\n",
      "ğŸ”¹ Available tuning result files:\n",
      "hyperparameter_results/results_lr0.0001_units256_epochs5.json\n",
      "hyperparameter_results/results_lr0.0005_units128_epochs5.json\n",
      "hyperparameter_results/results_lr0.001_units64_epochs5.json\n",
      "\n",
      "ğŸ”¹ Sample tuning result: {\n",
      "    \"learning_rate\": 0.0001,\n",
      "    \"num_units\": 256,\n",
      "    \"num_epochs\": 5,\n",
      "    \"epoch_results\": [\n",
      "        {\n",
      "            \"epoch\": 1,\n",
      "            \"loss\": 0.13530901074409485,\n",
      "            \"accuracy\": 0.8678833246231079\n",
      "        },\n",
      "        {\n",
      "            \"epoch\": 2,\n",
      "            \"loss\": 0.0818350613117218,\n",
      "            \"accuracy\": 0.9307666420936584\n",
      "        },\n",
      "        {\n",
      "            \"epoch\": 3,\n",
      "            \"loss\": 0.05850837007164955,\n",
      "            \"accuracy\": 0.9444166421890259\n",
      "        },\n",
      "        {\n",
      "            \"epoch\": 4,\n",
      "            \"loss\": 0.04567995294928551,\n",
      "            \"accuracy\": 0.9529333114624023\n",
      "        },\n",
      "        {\n",
      "            \"epoch\": 5,\n",
      "            \"loss\": 0.037972480058670044,\n",
      "            \"accuracy\": 0.9598833322525024\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "\n",
    "# Define directory to store tuning results\n",
    "results_dir = \"hyperparameter_results\"\n",
    "os.makedirs(results_dir, exist_ok=True)  # Create directory if it doesn't exist\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize pixel values (0 to 1)\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Create a batched dataset\n",
    "batch_size = 32\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
    "\n",
    "def tune_hyperparameters(learning_rate, num_units, num_epochs):\n",
    "    \"\"\"\n",
    "    Train a simple neural network with specified hyperparameters\n",
    "    and save results to a JSON file.\n",
    "    \n",
    "    Parameters:\n",
    "        learning_rate (float): Learning rate for the optimizer.\n",
    "        num_units (int): Number of neurons in the hidden layer.\n",
    "        num_epochs (int): Number of training epochs.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the model with hyperparameters\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(28, 28)),  # Flatten 28x28 images\n",
    "        Dense(num_units, activation='relu'),  # Hidden layer with tunable units\n",
    "        Dense(10)  # Output layer with 10 neurons (digits 0-9)\n",
    "    ])\n",
    "\n",
    "    # Define loss, optimizer, and metric\n",
    "    loss_fn = SparseCategoricalCrossentropy(from_logits=True)\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    accuracy_metric = SparseCategoricalAccuracy()\n",
    "\n",
    "    # Training loop\n",
    "    results = {\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"num_units\": num_units,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"epoch_results\": []\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nğŸ”¹ Start of Epoch {epoch + 1}\")\n",
    "\n",
    "        # Reset accuracy at the start of each epoch\n",
    "        accuracy_metric.reset_state()\n",
    "\n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Forward pass\n",
    "                logits = model(x_batch_train, training=True)\n",
    "                loss_value = loss_fn(y_batch_train, logits)\n",
    "\n",
    "            # Compute and apply gradients\n",
    "            grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "            # Update accuracy\n",
    "            accuracy_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "        # Store epoch results\n",
    "        epoch_data = {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"loss\": float(loss_value.numpy()),\n",
    "            \"accuracy\": float(accuracy_metric.result().numpy())\n",
    "        }\n",
    "        results[\"epoch_results\"].append(epoch_data)\n",
    "\n",
    "        # Print epoch summary\n",
    "        print(f\"ğŸ”¹ End of Epoch {epoch + 1}: Loss = {epoch_data['loss']:.4f}, Accuracy = {epoch_data['accuracy']:.4f}\")\n",
    "\n",
    "    # Save results as a JSON file\n",
    "    results_filename = f\"results_lr{learning_rate}_units{num_units}_epochs{num_epochs}.json\"\n",
    "    results_path = os.path.join(results_dir, results_filename)\n",
    "\n",
    "    with open(results_path, \"w\") as json_file:\n",
    "        json.dump(results, json_file, indent=4)\n",
    "\n",
    "    print(f\"âœ… Hyperparameter tuning results saved to {results_path}\")\n",
    "\n",
    "# Define different hyperparameter combinations to test\n",
    "hyperparameter_configs = [\n",
    "    {\"learning_rate\": 0.001, \"num_units\": 64, \"num_epochs\": 5},\n",
    "    {\"learning_rate\": 0.0005, \"num_units\": 128, \"num_epochs\": 5},\n",
    "    {\"learning_rate\": 0.0001, \"num_units\": 256, \"num_epochs\": 5}\n",
    "]\n",
    "\n",
    "# Run tuning for each hyperparameter set\n",
    "for config in hyperparameter_configs:\n",
    "    tune_hyperparameters(config[\"learning_rate\"], config[\"num_units\"], config[\"num_epochs\"])\n",
    "\n",
    "import glob\n",
    "\n",
    "# Get all JSON result files\n",
    "result_files = glob.glob(os.path.join(results_dir, \"*.json\"))\n",
    "\n",
    "# Print available result files\n",
    "print(\"\\nğŸ”¹ Available tuning result files:\")\n",
    "for file in result_files:\n",
    "    print(file)\n",
    "\n",
    "# Load and display one result file\n",
    "with open(result_files[0], \"r\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "    print(\"\\nğŸ”¹ Sample tuning result:\", json.dumps(data, indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click here for solution</summary> </br>\n",
    "\n",
    "```python\n",
    "!pip install keras-tuner\n",
    "!pip install scikit-learn\n",
    "\n",
    "import json\n",
    "import os\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Step 1: Load your dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Step 2: Define the model-building function\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    # Tune the number of units in the first Dense layer\n",
    "    model.add(Dense(units=hp.Int('units', min_value=32, max_value=512, step=32),\n",
    "                    activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Binary classification example\n",
    "    model.compile(optimizer=Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='LOG')),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Step 3: Initialize a Keras Tuner RandomSearch tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,  # Set the number of trials\n",
    "    executions_per_trial=1,  # Set how many executions per trial\n",
    "    directory='tuner_results',  # Directory for saving logs\n",
    "    project_name='hyperparam_tuning'\n",
    ")\n",
    "\n",
    "# Step 4: Run the tuner search (make sure the data is correct)\n",
    "tuner.search(X_train, y_train, validation_data=(X_val, y_val), epochs=5)\n",
    "\n",
    "# Step 5: Save the tuning results as JSON files\n",
    "try:\n",
    "    for i in range(10):\n",
    "        # Fetch the best hyperparameters from the tuner\n",
    "        best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "        \n",
    "        # Results dictionary to save hyperparameters and score\n",
    "        results = {\n",
    "            \"trial\": i + 1,\n",
    "            \"hyperparameters\": best_hps.values,  # Hyperparameters tuned in this trial\n",
    "            \"score\": None  # Add any score or metrics if available\n",
    "        }\n",
    "\n",
    "        # Save the results as JSON\n",
    "        with open(os.path.join('tuning_results', f\"trial_{i + 1}.json\"), \"w\") as f:\n",
    "            json.dump(results, f)\n",
    "\n",
    "except IndexError:\n",
    "    print(\"Tuning process has not completed or no results available.\")\n",
    " ```   \n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Explanation of Hyperparameter Tuning\n",
    "\n",
    "**Addition to Explanation:** Add a note explaining the purpose of num_trials in the hyperparameter tuning context:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click here for solution</summary> </br>\n",
    "\n",
    "```python\n",
    "Explanation: \"num_trials specifies the number of top hyperparameter sets to return. Setting num_trials=1 means that it will return only the best set of hyperparameters found during the tuning process.\"\n",
    " ```   \n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: \n",
    "\n",
    "Congratulations on completing this lab! You have now successfully created, trained, and evaluated a simple neural network model using the Keras Functional API. This foundational knowledge will allow you to build more complex models and explore advanced functionalities in Keras. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright Â© IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "48a1eb2565c8b635156cd21708473ccadb84e292e93f3530a9d5223b7590344e"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
